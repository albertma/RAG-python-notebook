{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42842a92",
   "metadata": {},
   "source": [
    "## 增强型 RAG 的相关分段提取 （RSE）\n",
    "\n",
    "在本笔记本中，我们实施了相关片段提取 （RSE） 技术来提高 RAG 系统中的上下文质量。我们不是简单地检索一组孤立的块，而是识别和重建连续的文本片段，为我们的语言模型提供更好的上下文。\n",
    "\n",
    "### 关键概念\n",
    "\n",
    "相关块往往在文档中聚集在一起。通过识别这些集群并保持它们的连续性，我们为 LLM 提供了更连贯的背景。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf49d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad9c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Get the page\n",
    "        text = page.get_text(\"text\")  # Extract text from the page\n",
    "        all_text += text  # Append the extracted text to the all_text string\n",
    "\n",
    "    return all_text  # Return the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b9cedc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=800, overlap=0):\n",
    "    \"\"\"\n",
    "    Split text into non-overlapping chunks.\n",
    "    For RSE, we typically want non-overlapping chunks so we can reconstruct segments properly.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to chunk\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of text chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    # Simple character-based chunking\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunk = text[i:i + chunk_size]\n",
    "        if chunk:  # Ensure we don't add empty chunks\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0911bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.siliconflow.cn/v1/\",\n",
    "    api_key=os.getenv(\"SILLICONFLOW_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cd25aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A lightweight vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self, dimension=1536):\n",
    "        \"\"\"\n",
    "        Initialize the vector store.\n",
    "        \n",
    "        Args:\n",
    "            dimension (int): Dimension of embeddings\n",
    "        \"\"\"\n",
    "        self.dimension = dimension\n",
    "        self.vectors = []\n",
    "        self.documents = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_documents(self, documents, vectors=None, metadata=None):\n",
    "        \"\"\"\n",
    "        Add documents to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            documents (List[str]): List of document chunks\n",
    "            vectors (List[List[float]], optional): List of embedding vectors\n",
    "            metadata (List[Dict], optional): List of metadata dictionaries\n",
    "        \"\"\"\n",
    "        if vectors is None:\n",
    "            vectors = [None] * len(documents)\n",
    "        \n",
    "        if metadata is None:\n",
    "            metadata = [{} for _ in range(len(documents))]\n",
    "        \n",
    "        for doc, vec, meta in zip(documents, vectors, metadata):\n",
    "            self.documents.append(doc)\n",
    "            self.vectors.append(vec)\n",
    "            self.metadata.append(meta)\n",
    "    \n",
    "    def search(self, query_vector, top_k=5):\n",
    "        \"\"\"\n",
    "        Search for most similar documents.\n",
    "        \n",
    "        Args:\n",
    "            query_vector (List[float]): Query embedding vector\n",
    "            top_k (int): Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: List of results with documents, scores, and metadata\n",
    "        \"\"\"\n",
    "        if not self.vectors or not self.documents:\n",
    "            return []\n",
    "        \n",
    "        # Convert query vector to numpy array\n",
    "        query_array = np.array(query_vector)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            if vector is not None:\n",
    "                # Compute cosine similarity\n",
    "                similarity = np.dot(query_array, vector) / (\n",
    "                    np.linalg.norm(query_array) * np.linalg.norm(vector)\n",
    "                )\n",
    "                similarities.append((i, similarity))\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get top-k results\n",
    "        results = []\n",
    "        for i, score in similarities[:top_k]:\n",
    "            results.append({\n",
    "                \"document\": self.documents[i],\n",
    "                \"score\": float(score),\n",
    "                \"metadata\": self.metadata[i]\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51152e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=\"BAAI/bge-m3\"):\n",
    "    \"\"\"\n",
    "    Generate embeddings for texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): List of texts to embed\n",
    "        model (str): Embedding model to use\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: List of embedding vectors\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        return []  # Return an empty list if no texts are provided\n",
    "        \n",
    "    # Process in batches if the list is long\n",
    "    batch_size = 100  # Adjust based on your API limits\n",
    "    all_embeddings = []  # Initialize a list to store all embeddings\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]  # Get the current batch of texts\n",
    "        \n",
    "        # Create embeddings for the current batch using the specified model\n",
    "        response = client.embeddings.create(\n",
    "            input=batch,\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        # Extract embeddings from the response\n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n",
    "        \n",
    "    return all_embeddings  # Return the list of all embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2176d5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=800):\n",
    "    \"\"\"\n",
    "    Process a document for use with RSE.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[str], SimpleVectorStore, Dict]: Chunks, vector store, and document info\n",
    "    \"\"\"\n",
    "    print(\"Extracting text from document...\")\n",
    "    # Extract text from the PDF file\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"Chunking text into non-overlapping segments...\")\n",
    "    # Chunk the extracted text into non-overlapping segments\n",
    "    chunks = chunk_text(text, chunk_size=chunk_size, overlap=0)\n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    \n",
    "    print(\"Generating embeddings for chunks...\")\n",
    "    # Generate embeddings for the text chunks\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # Create an instance of the SimpleVectorStore\n",
    "    vector_store = SimpleVectorStore()\n",
    "    \n",
    "    # Add documents with metadata (including chunk index for later reconstruction)\n",
    "    metadata = [{\"chunk_index\": i, \"source\": pdf_path} for i in range(len(chunks))]\n",
    "    vector_store.add_documents(chunks, chunk_embeddings, metadata)\n",
    "    \n",
    "    # Track original document structure for segment reconstruction\n",
    "    doc_info = {\n",
    "        \"chunks\": chunks,\n",
    "        \"source\": pdf_path,\n",
    "    }\n",
    "    \n",
    "    return chunks, vector_store, doc_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c822bb2c",
   "metadata": {},
   "source": [
    "# RSE (Relative Segment Extraction) \n",
    "计算文档中每个块的值，找到最相似的块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "186f6718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_values(query, chunks, vector_store, irrelevant_chunk_penalty=0.2):\n",
    "    # 计算chunk的值\n",
    "    query_embedding = create_embeddings([query])[0]\n",
    "    \n",
    "    # 计算所有chunk的向量表示\n",
    "    num_chunks = len(chunks)\n",
    "    results = vector_store.search(query_embedding, top_k=num_chunks)\n",
    "    \n",
    "    # create a mapping from chunk index to relevance score\n",
    "    relevance_scores = {result[\"metadata\"][\"chunk_index\"]: result[\"score\"] for result in results}\n",
    "    \n",
    "    # Calculate chunk values (relevance score - irrelevant chunk penalty)\n",
    "    chunk_values = []\n",
    "    for i in range(num_chunks):\n",
    "        chunk_value = relevance_scores.get(i, 0) - irrelevant_chunk_penalty\n",
    "        chunk_values.append(chunk_value)\n",
    "    \n",
    "    return chunk_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52628b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_segments(chunk_values, max_segment_length=20, \n",
    "                       total_max_length=30, min_segment_value=0.2):\n",
    "    \"\"\"\n",
    "    Find the best segments using a variant of the maximum sum subarray algorithm.\n",
    "    \n",
    "    Args:\n",
    "        chunk_values (List[float]): Values for each chunk\n",
    "        max_segment_length (int): Maximum length of a single segment\n",
    "        total_max_length (int): Maximum total length across all segments\n",
    "        min_segment_value (float): Minimum value for a segment to be considered\n",
    "        \n",
    "    Returns:\n",
    "        List[Tuple[int, int]]: List of (start, end) indices for best segments\n",
    "    \"\"\"\n",
    "    print(\"Finding optimal continuous text segments...\")\n",
    "    \n",
    "    best_segments = []\n",
    "    segment_scores = []\n",
    "    total_included_chunks = 0\n",
    "    \n",
    "    # Keep finding segments until we hit our limits\n",
    "    while total_included_chunks < total_max_length:\n",
    "        best_score = min_segment_value  # Minimum threshold for a segment\n",
    "        best_segment = None\n",
    "        \n",
    "        # Try each possible starting position\n",
    "        for start in range(len(chunk_values)):\n",
    "            # Skip if this start position is already in a selected segment\n",
    "            if any(start >= s[0] and start < s[1] for s in best_segments):\n",
    "                continue\n",
    "                \n",
    "            # Try each possible segment length\n",
    "            for length in range(1, min(max_segment_length, len(chunk_values) - start) + 1):\n",
    "                end = start + length\n",
    "                \n",
    "                # Skip if end position is already in a selected segment\n",
    "                if any(end > s[0] and end <= s[1] for s in best_segments):\n",
    "                    continue\n",
    "                \n",
    "                # Calculate segment value as sum of chunk values\n",
    "                segment_value = sum(chunk_values[start:end])\n",
    "                \n",
    "                # Update best segment if this one is better\n",
    "                if segment_value > best_score:\n",
    "                    best_score = segment_value\n",
    "                    best_segment = (start, end)\n",
    "        \n",
    "        # If we found a good segment, add it\n",
    "        if best_segment:\n",
    "            best_segments.append(best_segment)\n",
    "            segment_scores.append(best_score)\n",
    "            total_included_chunks += best_segment[1] - best_segment[0]\n",
    "            print(f\"Found segment {best_segment} with score {best_score:.4f}\")\n",
    "        else:\n",
    "            # No more good segments to find\n",
    "            break\n",
    "    \n",
    "    # Sort segments by their starting position for readability\n",
    "    best_segments = sorted(best_segments, key=lambda x: x[0])\n",
    "    \n",
    "    return best_segments, segment_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a32d1c",
   "metadata": {},
   "source": [
    "## 重构Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcc8c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_segments(chunks, best_segments):\n",
    "    \"\"\"\n",
    "    Reconstruct text segments based on chunk indices.\n",
    "    \n",
    "    Args:\n",
    "        chunks (List[str]): List of all document chunks\n",
    "        best_segments (List[Tuple[int, int]]): List of (start, end) indices for segments\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of reconstructed text segments\n",
    "    \"\"\"\n",
    "    reconstructed_segments = []  # Initialize an empty list to store the reconstructed segments\n",
    "    \n",
    "    for start, end in best_segments:\n",
    "        # Join the chunks in this segment to form the complete segment text\n",
    "        segment_text = \" \".join(chunks[start:end])\n",
    "        # Append the segment text and its range to the reconstructed_segments list\n",
    "        reconstructed_segments.append({\n",
    "            \"text\": segment_text,\n",
    "            \"segment_range\": (start, end),\n",
    "        })\n",
    "    \n",
    "    return reconstructed_segments  # Return the list of reconstructed text segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd50a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_segments_for_context(segments):\n",
    "    \"\"\"\n",
    "    Format segments into a context string for the LLM.\n",
    "    \n",
    "    Args:\n",
    "        segments (List[Dict]): List of segment dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted context text\n",
    "    \"\"\"\n",
    "    context = []  # Initialize an empty list to store the formatted context\n",
    "    \n",
    "    for i, segment in enumerate(segments):\n",
    "        # Create a header for each segment with its index and chunk range\n",
    "        segment_header = f\"SEGMENT {i+1} (Chunks {segment['segment_range'][0]}-{segment['segment_range'][1]-1}):\"\n",
    "        context.append(segment_header)  # Add the segment header to the context list\n",
    "        context.append(segment['text'])  # Add the segment text to the context list\n",
    "        context.append(\"-\" * 80)  # Add a separator line for readability\n",
    "    \n",
    "    # Join all elements in the context list with double newlines and return the result\n",
    "    return \"\\n\\n\".join(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7a9e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def generate_response(query, context, model=\"Qwen/Qwen3-14B\"):\n",
    "    \"\"\"\n",
    "    Generate a response based on the query and context.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): Context text from relevant segments\n",
    "        model (str): LLM model to use\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    print(\"Generating response using relevant segments as context...\")\n",
    "    \n",
    "    # Define the system prompt to guide the AI's behavior\n",
    "    system_prompt = \"\"\"\n",
    "    You are a helpful assistant that answers questions based on the provided context.\n",
    "    The context consists of document segments that have been retrieved as relevant to the user's query.\n",
    "    Use the information from these segments to provide a comprehensive and accurate answer.\n",
    "    If the context doesn't contain relevant information to answer the question, say so clearly.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the user prompt by combining the context and the query\n",
    "    user_prompt = f\"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Please provide a helpful answer based on the context provided.\n",
    "\"\"\"\n",
    "    \n",
    "    # Generate the response using the specified model\n",
    "    print(\"model: \", model)\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Return the generated response content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b7ff6c",
   "metadata": {},
   "source": [
    "## 完成RSE 流水线功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a236a8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_rse(pdf_path, query, chunk_size=800, irrelevant_chunk_penalty=0.2):\n",
    "    print(\"\\n=== STARTING RAG WITH RELEVANT SEGMENT EXTRACTION ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    chunks, vector_store, doc_info = process_document(pdf_path, chunk_size)\n",
    "    \n",
    "    print(\"\\n Calculating relevance scores and chunk values...\")\n",
    "    chunk_values = calculate_chunk_values(query, chunks, vector_store, irrelevant_chunk_penalty)\n",
    "    \n",
    "    best_segments, scores = find_best_segments(\n",
    "        chunk_values,\n",
    "        max_segment_length=20,\n",
    "        total_max_length=30,\n",
    "        min_segment_value=0.2\n",
    "    )\n",
    "    \n",
    "    print(\"\\n Reconstructing text segments from chunks...\")\n",
    "    segments = reconstruct_segments(chunks, best_segments)\n",
    "    context = format_segments_for_context(segments)\n",
    "    \n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"segments\": segments\n",
    "    }\n",
    "    print(\"\\n=== FINAL RESPONSE ===\")\n",
    "    print(f\"Response: {response}\")\n",
    "    return result\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1efb625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_top_k_retrieval(pdf_path, query, k=10, chunk_size=800):\n",
    "    print(\"\\n=== STARTING STANDARD TOP-K RETRIEVAL ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    chunks, vector_store, doc_info = process_document(pdf_path, chunk_size)\n",
    "    \n",
    "    print(\"\\n Calculating relevance scores and chunk values...\")\n",
    "    query_embedding = create_embeddings([query])[0]\n",
    "    \n",
    "    results = vector_store.search(query_embedding, top_k=k)\n",
    "    retrieved_chunks = [result[\"document\"] for result in results]\n",
    "    \n",
    "    # Format the retrieved chunks into a context string\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"CHUNK {i+1}:\\n{chunk}\" \n",
    "        for i, chunk in enumerate(retrieved_chunks)\n",
    "    ])\n",
    "    \n",
    "    # Generate a response from the language model using the context\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # Compile the result into a dictionary\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"chunks\": retrieved_chunks,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== FINAL RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26307d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_methods(pdf_path, query, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Compare RSE with standard top-k retrieval.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the document\n",
    "        query (str): User query\n",
    "        reference_answer (str, optional): Reference answer for evaluation\n",
    "    \"\"\"\n",
    "    print(\"\\n========= EVALUATION =========\\n\")\n",
    "    \n",
    "    # Run the RAG with Relevant Segment Extraction (RSE) method\n",
    "    rse_result = rag_with_rse(pdf_path, query)\n",
    "    \n",
    "    # Run the standard top-k retrieval method\n",
    "    standard_result = standard_top_k_retrieval(pdf_path, query)\n",
    "    \n",
    "    # If a reference answer is provided, evaluate the responses\n",
    "    if reference_answer:\n",
    "        print(\"\\n=== COMPARING RESULTS ===\")\n",
    "        \n",
    "        # Create an evaluation prompt to compare the responses against the reference answer\n",
    "        evaluation_prompt = f\"\"\"\n",
    "            Query: {query}\n",
    "\n",
    "            Reference Answer:\n",
    "            {reference_answer}\n",
    "\n",
    "            Response from Standard Retrieval:\n",
    "            {standard_result[\"response\"]}\n",
    "\n",
    "            Response from Relevant Segment Extraction:\n",
    "            {rse_result[\"response\"]}\n",
    "\n",
    "            Compare these two responses against the reference answer. Which one is:\n",
    "            1. More accurate and comprehensive\n",
    "            2. Better at addressing the user's query\n",
    "            3. Less likely to include irrelevant information\n",
    "\n",
    "            Explain your reasoning for each point.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Evaluating responses against reference answer...\")\n",
    "        \n",
    "        # Generate the evaluation using the specified model\n",
    "        evaluation = client.chat.completions.create(\n",
    "            model=\"Qwen/Qwen3-14B\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an objective evaluator of RAG system responses.\"},\n",
    "                {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Print the evaluation results\n",
    "        print(\"\\n=== EVALUATION RESULTS ===\")\n",
    "        print(evaluation.choices[0].message.content)\n",
    "    \n",
    "    # Return the results of both methods\n",
    "    return {\n",
    "        \"rse_result\": rse_result,\n",
    "        \"standard_result\": standard_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3cb64526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= EVALUATION =========\n",
      "\n",
      "\n",
      "=== STARTING RAG WITH RELEVANT SEGMENT EXTRACTION ===\n",
      "Query: What is 'Explainable AI' and why is it considered important?\n",
      "Extracting text from document...\n",
      "Chunking text into non-overlapping segments...\n",
      "Created 42 chunks\n",
      "Generating embeddings for chunks...\n",
      "\n",
      " Calculating relevance scores and chunk values...\n",
      "Finding optimal continuous text segments...\n",
      "Found segment (22, 42) with score 6.1448\n",
      "Found segment (0, 20) with score 5.8953\n",
      "\n",
      " Reconstructing text segments from chunks...\n",
      "Generating response using relevant segments as context...\n",
      "model:  Qwen/Qwen3-14B\n",
      "\n",
      "=== FINAL RESPONSE ===\n",
      "Response: \n",
      "\n",
      "**Explainable AI (XAI)** is a subfield of artificial intelligence focused on creating **transparent and understandable AI systems**. It aims to address the \"black box\" nature of many complex AI models, particularly deep learning systems, by providing insights into how decisions are made. This involves developing methods to explain the reasoning behind AI outputs, making its processes more interpretable for users and stakeholders.\n",
      "\n",
      "**Why it is considered important:**  \n",
      "1. **Trust and Accountability**: XAI enhances trust by allowing users to understand and verify AI decisions, ensuring accountability for outcomes, especially in critical domains like healthcare, finance, and law.  \n",
      "2. **Ethical and Fair Use**: By making AI systems explainable, it becomes easier to identify and mitigate biases or unfairness in decision-making processes.  \n",
      "3. **Regulatory Compliance**: As AI becomes more pervasive, regulations and governance frameworks require transparency to ensure responsible development and deployment.  \n",
      "4. **User Empowerment**: Users can better assess the reliability, fairness, and accuracy of AI systems when they have access to clear explanations, enabling informed decisions and reducing misuse.  \n",
      "5. **Error Detection and Safety**: Explainability helps in diagnosing errors, improving system safety, and addressing unintended consequences of AI behavior.  \n",
      "\n",
      "In summary, XAI is vital for ensuring that AI systems are not only effective but also **ethical, reliable, and aligned with human values**, fostering broader acceptance and responsible innovation.\n",
      "\n",
      "=== STARTING STANDARD TOP-K RETRIEVAL ===\n",
      "Query: What is 'Explainable AI' and why is it considered important?\n",
      "Extracting text from document...\n",
      "Chunking text into non-overlapping segments...\n",
      "Created 42 chunks\n",
      "Generating embeddings for chunks...\n",
      "\n",
      " Calculating relevance scores and chunk values...\n",
      "Generating response using relevant segments as context...\n",
      "model:  Qwen/Qwen3-14B\n",
      "\n",
      "=== FINAL RESPONSE ===\n",
      "\n",
      "\n",
      "**Explainable AI (XAI)** refers to techniques and methods designed to make artificial intelligence (AI) systems more transparent and understandable. Its primary goal is to provide insights into how AI systems arrive at their decisions, enabling users to assess the fairness, accuracy, and reliability of these systems. \n",
      "\n",
      "### Why is XAI considered important?\n",
      "1. **Building Trust**: By making AI decisions interpretable, XAI helps users (including developers, deployers, and end-users) understand and trust AI systems. This is critical for adoption in high-stakes domains like healthcare, finance, and law.\n",
      "2. **Ensuring Fairness and Accountability**: XAI allows stakeholders to identify and address biases or errors in AI outcomes, ensuring systems align with ethical principles and societal values. It supports accountability by clarifying who is responsible for AI-driven decisions.\n",
      "3. **Addressing \"Black Box\" Challenges**: Many AI systems, particularly complex models like deep learning networks, operate as \"black boxes,\" making their decision-making processes opaque. XAI enhances transparency, reducing risks of unintended consequences and improving oversight.\n",
      "4. **Compliance with Regulations**: As data privacy and ethical AI frameworks (e.g., GDPR, ethical AI principles) gain prominence, XAI helps organizations comply with requirements for transparency and responsible AI use.\n",
      "5. **Enhancing Robustness**: By enabling scrutiny of AI behavior, XAI supports the identification of vulnerabilities, improving the reliability and safety of AI systems.\n",
      "\n",
      "In summary, XAI is vital for fostering trust, ensuring ethical use, and addressing the inherent opacity of many AI systems, making it a cornerstone of responsible AI development.\n",
      "\n",
      "=== COMPARING RESULTS ===\n",
      "Evaluating responses against reference answer...\n",
      "\n",
      "=== EVALUATION RESULTS ===\n",
      "\n",
      "\n",
      "### 1. **More accurate and comprehensive**  \n",
      "**Standard Retrieval** wins this category.  \n",
      "- **Accuracy**: The standard retrieval explicitly mentions **\"black box\" challenges**, which directly aligns with the reference answer's emphasis on transparency. It also incorporates specific regulatory frameworks like the **General Data Protection Regulation (GDPR)** and **ethical AI principles**, adding precision that the other response lacks.  \n",
      "- **Comprehensiveness**: The standard retrieval covers all five key reasons from the reference answer (trust, fairness/accountability, black box challenges, regulatory compliance, and robustness) and extends the discussion of regulatory compliance with concrete examples (e.g., GDPR). The relevant segment extraction answer merges **trust and accountability** into one point and omits the explicit mention of **\"ensuring fairness\"** (though it does mention \"ethical and fair use\" in the second bullet point). The standard retrieval provides a clearer separation of these distinct concepts.  \n",
      "\n",
      "---\n",
      "\n",
      "### 2. **Better at addressing the user's query**  \n",
      "**Standard Retrieval** also wins here.  \n",
      "- The user’s query is twofold: **\"What is Explainable AI?\"** and **\"Why is it considered important?\"** The standard retrieval answers both parts with **a structured definition** and **five detailed bullet points** that directly expand on the reasoning outlined in the reference answer.  \n",
      "- The relevant segment extraction answer is similarly structured but **groups \"trust and accountability\" together**, which slightly oversimplifies the distinction between these two importance factors. Additionally, it uses variations like **\"user empowerment\"** and **\"ethical and fair use\"** rather than explicitly naming **fairness** and **accountability** as separate points, which could make it less aligned with the user’s explicit request for clarity on \"importance.\"**  \n",
      "- The standard retrieval’s mention of **\"complex models like deep learning networks\"** in the definition and **\"unintended consequences\"** in the \"black box\" section provides **contextual depth** that the user might find valuable, even though it’s slightly tangential.  \n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Less likely to include irrelevant information**  \n",
      "**Relevant Segment Extraction** is slightly better here.  \n",
      "- The standard retrieval includes **specific examples of regulations (e.g., GDPR)** and **a broader discussion of ethical AI frameworks**, which are **relevant** but somewhat **tangential** to the core definition of XAI. The reference answer emphasizes **transparency and fairness** without diving into regulatory specifics, so including GDPR adds context that may not be essential for a foundational answer.  \n",
      "- The relevant segment extraction focuses on **the core importance factors** (trust, fairness, compliance, empowerment, safety) without introducing specific regulatory names or extending into implementation details (e.g., error detection methods). Its language is **more generalized** and **directly aligned** with the reference answer’s emphasis on **ethical use, trust, and accountability**.  \n",
      "- While the standard retrieval is accurate, its inclusion of **GDPR** and **ethical AI frameworks** could be seen as **slightly expanding beyond the scope** of the query if the user sought a basic explanation rather than regulatory context. The relevant segment extraction avoids such specifics and stays closer to the reference answer’s focus.  \n",
      "\n",
      "---\n",
      "\n",
      "### Summary:  \n",
      "- **Accuracy & Comprehensiveness**: Standard Retrieval (covers all reference points, includes specific examples).  \n",
      "- **Adressing the Query**: Standard Retrieval (more explicit alignment with the reference’s structure and reasoning).  \n",
      "- **Irrelevant Information**: Relevant Segment Extraction (stays focused on core importance factors without regulatory specifics).  \n",
      "\n",
      "The **standard retrieval** is slightly more accurate and comprehensive, but the **relevant segment extraction** is more streamlined and less likely to introduce tangential details unless the user explicitly requested them.\n"
     ]
    }
   ],
   "source": [
    "# Load the validation data from a JSON file\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the first query from the validation data\n",
    "query = data[0]['question']\n",
    "\n",
    "# Extract the reference answer from the validation data\n",
    "reference_answer = data[0]['ideal_answer']\n",
    "\n",
    "# pdf_path\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate_methods(pdf_path, query, reference_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
