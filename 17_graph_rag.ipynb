{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph RAG：图增强检索增强生成\n",
    "\n",
    "在本笔记本中，我实现了 Graph RAG，这是一种通过将知识组织为连接的图形而不是平面文档集合来增强传统 RAG 系统的技术。这允许系统导航相关概念并检索比标准向量相似性方法更相关的上下文信息。\n",
    "\n",
    "Graph RAG 的主要优势\n",
    "\n",
    "- 保留信息之间的关系\n",
    "- 支持遍历连接的概念以查找相关上下文\n",
    "- 改进了对复杂的多部分查询的处理\n",
    "- 通过可视化的知识路径提供更好的可解释性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://api.siliconflow.cn/v1/\",\n",
    "    api_key=os.getenv(\"SILLICONFLOW_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text content from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text content\n",
    "    \"\"\"\n",
    "    print(f\"Extracting text from {pdf_path}...\")  # Print the path of the PDF being processed\n",
    "    pdf_document = fitz.open(pdf_path)  # Open the PDF file using PyMuPDF\n",
    "    text = \"\"  # Initialize an empty string to store the extracted text\n",
    "    \n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document[page_num]  # Get the page object\n",
    "        text += page.get_text()  # Extract text from the page and append to the text string\n",
    "    \n",
    "    return text  # Return the extracted text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to chunk\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of chunks with metadata\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Iterate over the text with a step size of (chunk_size - overlap)\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        # Extract a chunk of text from the current position\n",
    "        chunk_text = text[i:i + chunk_size]\n",
    "        \n",
    "        # Ensure we don't add empty chunks\n",
    "        if chunk_text:\n",
    "            # Append the chunk with its metadata to the list\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,  # The chunk of text\n",
    "                \"index\": len(chunks),  # The index of the chunk\n",
    "                \"start_pos\": i,  # The starting position of the chunk in the original text\n",
    "                \"end_pos\": i + len(chunk_text)  # The ending position of the chunk in the original text\n",
    "            })\n",
    "    \n",
    "    # Print the number of chunks created\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    return chunks  # Return the list of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=\"netease-youdao/bce-embedding-base_v1\"):\n",
    "    \"\"\"\n",
    "    Create embeddings for the given texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): Input texts\n",
    "        model (str): Embedding model name\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: Embedding vectors\n",
    "    \"\"\"\n",
    "    # Handle empty input\n",
    "    if not texts:\n",
    "        return []\n",
    "        \n",
    "    # Process in batches if needed (OpenAI API limits)\n",
    "    batch_size = 32\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Iterate over the input texts in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]  # Get the current batch of texts\n",
    "        \n",
    "        # Create embeddings for the current batch\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=batch\n",
    "        )\n",
    "        \n",
    "        # Extract embeddings from the response\n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n",
    "    \n",
    "    return all_embeddings  # Return all embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_concepts(text):\n",
    "    \"\"\"\n",
    "    Extract key concepts from text using OpenAI's API.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to extract concepts from\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of concepts\n",
    "    \"\"\"\n",
    "    # System message to instruct the model on what to do\n",
    "    system_message = \"\"\"Extract key concepts and entities from the provided text.\n",
    "Return ONLY a list of 5-10 key terms, entities, or concepts that are most important in this text.\n",
    "Format your response as a JSON array of strings.\"\"\"\n",
    "\n",
    "    # Make a request to the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Qwen/Qwen3-14B\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": f\"Extract key concepts from:\\n\\n{text[:3000]}\"}  # Limit for API\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Parse concepts from the response\n",
    "        concepts_json = json.loads(response.choices[0].message.content)\n",
    "        concepts = concepts_json.get(\"concepts\", [])\n",
    "        if not concepts and \"concepts\" not in concepts_json:\n",
    "            # Try to get any array in the response\n",
    "            for key, value in concepts_json.items():\n",
    "                if isinstance(value, list):\n",
    "                    concepts = value\n",
    "                    break\n",
    "        return concepts\n",
    "    except (json.JSONDecodeError, AttributeError):\n",
    "        # Fallback if JSON parsing fails\n",
    "        content = response.choices[0].message.content\n",
    "        # Try to extract anything that looks like a list\n",
    "        matches = re.findall(r'\\[(.*?)\\]', content, re.DOTALL)\n",
    "        if matches:\n",
    "            items = re.findall(r'\"([^\"]*)\"', matches[0])\n",
    "            return items\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_knowledge_graph(chunks):\n",
    "    \"\"\"\n",
    "    Build a knowledge graph from text chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks (List[Dict]): List of text chunks with metadata\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[nx.Graph, List[np.ndarray]]: The knowledge graph and chunk embeddings\n",
    "    \"\"\"\n",
    "    print(\"Building knowledge graph...\")\n",
    "    \n",
    "    # Create a graph\n",
    "    graph = nx.Graph()\n",
    "    \n",
    "    # Extract chunk texts\n",
    "    texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    \n",
    "    # Create embeddings for all chunks\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    embeddings = create_embeddings(texts)\n",
    "    \n",
    "    # Add nodes to the graph\n",
    "    print(\"Adding nodes to the graph...\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Extract concepts from the chunk\n",
    "        print(f\"Extracting concepts for chunk {i+1}/{len(chunks)}...\")\n",
    "        concepts = extract_concepts(chunk[\"text\"])\n",
    "        \n",
    "        # Add node with attributes\n",
    "        graph.add_node(i, \n",
    "                      text=chunk[\"text\"], \n",
    "                      concepts=concepts,\n",
    "                      embedding=embeddings[i])\n",
    "    \n",
    "    # Connect nodes based on shared concepts\n",
    "    print(\"Creating edges between nodes...\")\n",
    "    for i in range(len(chunks)):\n",
    "        node_concepts = set(graph.nodes[i][\"concepts\"])\n",
    "        \n",
    "        for j in range(i + 1, len(chunks)):\n",
    "            # Calculate concept overlap\n",
    "            other_concepts = set(graph.nodes[j][\"concepts\"])\n",
    "            shared_concepts = node_concepts.intersection(other_concepts)\n",
    "            \n",
    "            # If they share concepts, add an edge\n",
    "            if shared_concepts:\n",
    "                # Calculate semantic similarity using embeddings\n",
    "                similarity = np.dot(embeddings[i], embeddings[j]) / (np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j]))\n",
    "                \n",
    "                # Calculate edge weight based on concept overlap and semantic similarity\n",
    "                concept_score = len(shared_concepts) / min(len(node_concepts), len(other_concepts))\n",
    "                edge_weight = 0.7 * similarity + 0.3 * concept_score\n",
    "                \n",
    "                # Only add edges with significant relationship\n",
    "                if edge_weight > 0.6:\n",
    "                    graph.add_edge(i, j, \n",
    "                                  weight=edge_weight,\n",
    "                                  similarity=similarity,\n",
    "                                  shared_concepts=list(shared_concepts))\n",
    "    \n",
    "    print(f\"Knowledge graph built with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges\")\n",
    "    return graph, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图形遍历和查询处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_graph(query, graph, embeddings, top_k=5, max_depth=3):\n",
    "    \"\"\"\n",
    "    Traverse the knowledge graph to find relevant information for the query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's question\n",
    "        graph (nx.Graph): The knowledge graph\n",
    "        embeddings (List): List of node embeddings\n",
    "        top_k (int): Number of initial nodes to consider\n",
    "        max_depth (int): Maximum traversal depth\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Relevant information from graph traversal\n",
    "    \"\"\"\n",
    "    print(f\"Traversing graph for query: {query}\")\n",
    "    \n",
    "    # Get query embedding\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # Calculate similarity between query and all nodes\n",
    "    similarities = []\n",
    "    for i, node_embedding in enumerate(embeddings):\n",
    "        similarity = np.dot(query_embedding, node_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(node_embedding))\n",
    "        similarities.append((i, similarity[0]))\n",
    "    \n",
    "    # Sort by similarity (descending)\n",
    "    similarities.sort(key=lambda x: float(x[1]), reverse=True)\n",
    "    \n",
    "    # Get top-k most similar nodes as starting points\n",
    "    starting_nodes = [node for node, _ in similarities[:top_k]]\n",
    "    print(f\"Starting traversal from {len(starting_nodes)} nodes\")\n",
    "    \n",
    "    # Initialize traversal\n",
    "    visited = set()  # Set to keep track of visited nodes\n",
    "    traversal_path = []  # List to store the traversal path\n",
    "    results = []  # List to store the results\n",
    "    \n",
    "    # Use a priority queue for traversal\n",
    "    queue = []\n",
    "    for node in starting_nodes:\n",
    "        heapq.heappush(queue, (-similarities[node][1], node))  # Negative for max-heap\n",
    "    \n",
    "    # Traverse the graph using a modified breadth-first search with priority\n",
    "    while queue and len(results) < (top_k * 3):  # Limit results to top_k * 3\n",
    "        _, node = heapq.heappop(queue)\n",
    "        \n",
    "        if node in visited:\n",
    "            continue\n",
    "        \n",
    "        # Mark as visited\n",
    "        visited.add(node)\n",
    "        traversal_path.append(node)\n",
    "        \n",
    "        # Add current node's text to results\n",
    "        results.append({\n",
    "            \"text\": graph.nodes[node][\"text\"],\n",
    "            \"concepts\": graph.nodes[node][\"concepts\"],\n",
    "            \"node_id\": node\n",
    "        })\n",
    "        \n",
    "        # Explore neighbors if we haven't reached max depth\n",
    "        if len(traversal_path) < max_depth:\n",
    "            neighbors = [(neighbor, graph[node][neighbor][\"weight\"]) \n",
    "                        for neighbor in graph.neighbors(node)\n",
    "                        if neighbor not in visited]\n",
    "            \n",
    "            # Add neighbors to queue based on edge weight\n",
    "            for neighbor, weight in sorted(neighbors, key=lambda x: x[1], reverse=True):\n",
    "                heapq.heappush(queue, (-weight, neighbor))\n",
    "    \n",
    "    print(f\"图形遍历，找到 {len(results)} 个相关块\")\n",
    "    return results, traversal_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context_chunks):\n",
    "    \"\"\"\n",
    "    Generate a response using the retrieved context.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's question\n",
    "        context_chunks (List[Dict]): Relevant chunks from graph traversal\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Extract text from each chunk in the context\n",
    "    context_texts = [chunk[\"text\"] for chunk in context_chunks]\n",
    "    \n",
    "    # Combine the extracted texts into a single context string, separated by \"---\"\n",
    "    combined_context = \"\\n\\n---\\n\\n\".join(context_texts)\n",
    "    \n",
    "    # Define the maximum allowed length for the context (OpenAI limit)\n",
    "    max_context = 14000\n",
    "    \n",
    "    # Truncate the combined context if it exceeds the maximum length\n",
    "    if len(combined_context) > max_context:\n",
    "        combined_context = combined_context[:max_context] + \"... [truncated]\"\n",
    "    \n",
    "    # Define the system message to guide the AI assistant\n",
    "    system_message = \"\"\"You are a helpful AI assistant. Answer the user's question based on the provided context.\n",
    "        If the information is not in the context, say so. Refer to specific parts of the context in your answer when possible.\"\"\"\n",
    "\n",
    "    # Generate the response using the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Qwen/Qwen3-14B\",  # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{combined_context}\\n\\nQuestion: {query}\"}  # User message with context and query\n",
    "        ],\n",
    "        temperature=0.2  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated response content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_graph_traversal(graph, traversal_path):\n",
    "    \"\"\"\n",
    "    Visualize the knowledge graph and the traversal path.\n",
    "    \n",
    "    Args:\n",
    "        graph (nx.Graph): The knowledge graph\n",
    "        traversal_path (List): List of nodes in traversal order\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 10))  # Set the figure size\n",
    "    \n",
    "    # Define node colors, default to light blue\n",
    "    node_color = ['lightblue'] * graph.number_of_nodes()\n",
    "    \n",
    "    # Highlight traversal path nodes in light green\n",
    "    for node in traversal_path:\n",
    "        node_color[node] = 'lightgreen'\n",
    "    \n",
    "    # Highlight start node in green and end node in red\n",
    "    if traversal_path:\n",
    "        node_color[traversal_path[0]] = 'green'\n",
    "        node_color[traversal_path[-1]] = 'red'\n",
    "    \n",
    "    # Create positions for all nodes using spring layout\n",
    "    pos = nx.spring_layout(graph, k=0.5, iterations=50, seed=42)\n",
    "    \n",
    "    # Draw the graph nodes\n",
    "    nx.draw_networkx_nodes(graph, pos, node_color=node_color, node_size=500, alpha=0.8)\n",
    "    \n",
    "    # Draw edges with width proportional to weight\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        weight = data.get('weight', 1.0)\n",
    "        nx.draw_networkx_edges(graph, pos, edgelist=[(u, v)], width=weight*2, alpha=0.6)\n",
    "    \n",
    "    # Draw traversal path with red dashed lines\n",
    "    traversal_edges = [(traversal_path[i], traversal_path[i+1]) \n",
    "                      for i in range(len(traversal_path)-1)]\n",
    "    \n",
    "    nx.draw_networkx_edges(graph, pos, edgelist=traversal_edges, \n",
    "                          width=3, alpha=0.8, edge_color='red', \n",
    "                          style='dashed', arrows=True)\n",
    "    \n",
    "    # Add labels with the first concept for each node\n",
    "    labels = {}\n",
    "    for node in graph.nodes():\n",
    "        concepts = graph.nodes[node]['concepts']\n",
    "        label = concepts[0] if concepts else f\"Node {node}\"\n",
    "        labels[node] = f\"{node}: {label}\"\n",
    "    \n",
    "    nx.draw_networkx_labels(graph, pos, labels=labels, font_size=8)\n",
    "    \n",
    "    plt.title(\"Knowledge Graph with Traversal Path\")  # Set the plot title\n",
    "    plt.axis('off')  # Turn off the axis\n",
    "    plt.tight_layout()  # Adjust layout\n",
    "    plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Graph RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_rag_pipeline(pdf_path, query, chunk_size=1000, chunk_overlap=200, top_k=3):\n",
    "    \"\"\"\n",
    "    Complete Graph RAG pipeline from document to answer.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        query (str): The user's question\n",
    "        chunk_size (int): Size of text chunks\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        top_k (int): Number of top nodes to consider for traversal\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including answer and graph visualization data\n",
    "    \"\"\"\n",
    "    # Extract text from the PDF document\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Split the extracted text into overlapping chunks\n",
    "    chunks = chunk_text(text, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # Build a knowledge graph from the text chunks\n",
    "    graph, embeddings = build_knowledge_graph(chunks)\n",
    "    \n",
    "    # Traverse the knowledge graph to find relevant information for the query\n",
    "    relevant_chunks, traversal_path = traverse_graph(query, graph, embeddings, top_k)\n",
    "    \n",
    "    # Generate a response based on the query and the relevant chunks\n",
    "    response = generate_response(query, relevant_chunks)\n",
    "    \n",
    "    # Visualize the graph traversal path\n",
    "    visualize_graph_traversal(graph, traversal_path)\n",
    "    \n",
    "    # Return the query, response, relevant chunks, traversal path, and the graph\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"relevant_chunks\": relevant_chunks,\n",
    "        \"traversal_path\": traversal_path,\n",
    "        \"graph\": graph\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_graph_rag(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    Evaluate Graph RAG on multiple test queries.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        test_queries (List[str]): List of test queries\n",
    "        reference_answers (List[str], optional): Reference answers for comparison\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results\n",
    "    \"\"\"\n",
    "    # Extract text from PDF\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Split text into chunks\n",
    "    chunks = chunk_text(text)\n",
    "    \n",
    "    # Build knowledge graph (do this once for all queries)\n",
    "    graph, embeddings = build_knowledge_graph(chunks)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\n=== Evaluating Query {i+1}/{len(test_queries)} ===\")\n",
    "        print(f\"Query: {query}\")\n",
    "        \n",
    "        # Traverse graph to find relevant information\n",
    "        relevant_chunks, traversal_path = traverse_graph(query, graph, embeddings)\n",
    "        \n",
    "        # Generate response\n",
    "        response = generate_response(query, relevant_chunks)\n",
    "        \n",
    "        # Compare with reference answer if available\n",
    "        reference = None\n",
    "        comparison = None\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference = reference_answers[i]\n",
    "            comparison = compare_with_reference(response, reference, query)\n",
    "        \n",
    "        # Append results for the current query\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"reference_answer\": reference,\n",
    "            \"comparison\": comparison,\n",
    "            \"traversal_path_length\": len(traversal_path),\n",
    "            \"relevant_chunks_count\": len(relevant_chunks)\n",
    "        })\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nResponse: {response}\\n\")\n",
    "        if comparison:\n",
    "            print(f\"Comparison: {comparison}\\n\")\n",
    "    \n",
    "    # Return evaluation results and graph statistics\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"graph_stats\": {\n",
    "            \"nodes\": graph.number_of_nodes(),\n",
    "            \"edges\": graph.number_of_edges(),\n",
    "            \"avg_degree\": sum(dict(graph.degree()).values()) / graph.number_of_nodes()\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_reference(response, reference, query):\n",
    "    \"\"\"\n",
    "    Compare generated response with reference answer.\n",
    "    \n",
    "    Args:\n",
    "        response (str): Generated response\n",
    "        reference (str): Reference answer\n",
    "        query (str): Original query\n",
    "        \n",
    "    Returns:\n",
    "        str: Comparison analysis\n",
    "    \"\"\"\n",
    "    # System message to instruct the model on how to compare the responses\n",
    "    system_message = \"\"\"Compare the AI-generated response with the reference answer.\n",
    "        Evaluate based on: correctness, completeness, and relevance to the query.\n",
    "        Provide a brief analysis (2-3 sentences) of how well the generated response matches the reference.\"\"\"\n",
    "\n",
    "    # Construct the prompt with the query, AI-generated response, and reference answer\n",
    "    prompt = f\"\"\"\n",
    "        Query: {query}\n",
    "\n",
    "        AI-generated response:\n",
    "        {response}\n",
    "\n",
    "        Reference answer:\n",
    "        {reference}\n",
    "\n",
    "        How well does the AI response match the reference?\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a request to the OpenAI API to generate the comparison analysis\n",
    "    comparison = client.chat.completions.create(\n",
    "        model=\"Qwen/Qwen3-14B\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": prompt}  # User message with the prompt\n",
    "        ],\n",
    "        temperature=0.0  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated comparison analysis\n",
    "    return comparison.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估一个简单PDF上的图形RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from data/AI_Information.pdf...\n",
      "Created 42 text chunks\n",
      "Building knowledge graph...\n",
      "Creating embeddings for chunks...\n",
      "Adding nodes to the graph...\n",
      "Extracting concepts for chunk 1/42...\n",
      "Extracting concepts for chunk 2/42...\n",
      "Extracting concepts for chunk 3/42...\n",
      "Extracting concepts for chunk 4/42...\n",
      "Extracting concepts for chunk 5/42...\n",
      "Extracting concepts for chunk 6/42...\n",
      "Extracting concepts for chunk 7/42...\n",
      "Extracting concepts for chunk 8/42...\n",
      "Extracting concepts for chunk 9/42...\n",
      "Extracting concepts for chunk 10/42...\n",
      "Extracting concepts for chunk 11/42...\n",
      "Extracting concepts for chunk 12/42...\n",
      "Extracting concepts for chunk 13/42...\n",
      "Extracting concepts for chunk 14/42...\n",
      "Extracting concepts for chunk 15/42...\n",
      "Extracting concepts for chunk 16/42...\n",
      "Extracting concepts for chunk 17/42...\n",
      "Extracting concepts for chunk 18/42...\n",
      "Extracting concepts for chunk 19/42...\n",
      "Extracting concepts for chunk 20/42...\n",
      "Extracting concepts for chunk 21/42...\n",
      "Extracting concepts for chunk 22/42...\n",
      "Extracting concepts for chunk 23/42...\n",
      "Extracting concepts for chunk 24/42...\n",
      "Extracting concepts for chunk 25/42...\n",
      "Extracting concepts for chunk 26/42...\n",
      "Extracting concepts for chunk 27/42...\n",
      "Extracting concepts for chunk 28/42...\n",
      "Extracting concepts for chunk 29/42...\n",
      "Extracting concepts for chunk 30/42...\n",
      "Extracting concepts for chunk 31/42...\n",
      "Extracting concepts for chunk 32/42...\n",
      "Extracting concepts for chunk 33/42...\n",
      "Extracting concepts for chunk 34/42...\n",
      "Extracting concepts for chunk 35/42...\n",
      "Extracting concepts for chunk 36/42...\n",
      "Extracting concepts for chunk 37/42...\n",
      "Extracting concepts for chunk 38/42...\n",
      "Extracting concepts for chunk 39/42...\n",
      "Extracting concepts for chunk 40/42...\n",
      "Extracting concepts for chunk 41/42...\n",
      "Extracting concepts for chunk 42/42...\n",
      "Creating edges between nodes...\n",
      "Knowledge graph built with 42 nodes and 9 edges\n",
      "Traversing graph for query: What are the key applications of transformers in natural language processing?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only length-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mWhat are the key applications of transformers in natural language processing?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Execute the Graph RAG pipeline to process the document and answer the query\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m results = \u001b[43mgraph_rag_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Print the response generated from the Graph RAG system\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== ANSWER ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mgraph_rag_pipeline\u001b[39m\u001b[34m(pdf_path, query, chunk_size, chunk_overlap, top_k)\u001b[39m\n\u001b[32m     22\u001b[39m graph, embeddings = build_knowledge_graph(chunks)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Traverse the knowledge graph to find relevant information for the query\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m relevant_chunks, traversal_path = \u001b[43mtraverse_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Generate a response based on the query and the relevant chunks\u001b[39;00m\n\u001b[32m     28\u001b[39m response = generate_response(query, relevant_chunks)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mtraverse_graph\u001b[39m\u001b[34m(query, graph, embeddings, top_k, max_depth)\u001b[39m\n\u001b[32m     24\u001b[39m     similarities.append((i, similarity))\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Sort by similarity (descending)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43msimilarities\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Get top-k most similar nodes as starting points\u001b[39;00m\n\u001b[32m     30\u001b[39m starting_nodes = [node \u001b[38;5;28;01mfor\u001b[39;00m node, _ \u001b[38;5;129;01min\u001b[39;00m similarities[:top_k]]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mtraverse_graph.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     24\u001b[39m     similarities.append((i, similarity))\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Sort by similarity (descending)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m similarities.sort(key=\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Get top-k most similar nodes as starting points\u001b[39;00m\n\u001b[32m     30\u001b[39m starting_nodes = [node \u001b[38;5;28;01mfor\u001b[39;00m node, _ \u001b[38;5;129;01min\u001b[39;00m similarities[:top_k]]\n",
      "\u001b[31mTypeError\u001b[39m: only length-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "# Path to the PDF document containing AI information\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# Define an AI-related query for testing Graph RAG\n",
    "query = \"What are the key applications of transformers in natural language processing?\"\n",
    "\n",
    "# Execute the Graph RAG pipeline to process the document and answer the query\n",
    "results = graph_rag_pipeline(pdf_path, query)\n",
    "\n",
    "# Print the response generated from the Graph RAG system\n",
    "print(\"\\n=== ANSWER ===\")\n",
    "print(results[\"response\"])\n",
    "\n",
    "# Define a test query and reference answer for formal evaluation\n",
    "test_queries = [\n",
    "    \"How do transformers handle sequential data compared to RNNs?\"\n",
    "]\n",
    "\n",
    "# Reference answer for evaluation purposes\n",
    "reference_answers = [\n",
    "    \"Transformers handle sequential data differently from RNNs by using self-attention mechanisms instead of recurrent connections. This allows transformers to process all tokens in parallel rather than sequentially, capturing long-range dependencies more efficiently and enabling better parallelization during training. Unlike RNNs, transformers don't suffer from vanishing gradient problems with long sequences.\"\n",
    "]\n",
    "\n",
    "# Run formal evaluation of the Graph RAG system with the test query\n",
    "evaluation = evaluate_graph_rag(pdf_path, test_queries, reference_answers)\n",
    "\n",
    "# Print evaluation summary statistics\n",
    "print(\"\\n=== EVALUATION SUMMARY ===\")\n",
    "print(f\"Graph nodes: {evaluation['graph_stats']['nodes']}\")\n",
    "print(f\"Graph edges: {evaluation['graph_stats']['edges']}\")\n",
    "for i, result in enumerate(evaluation['results']):\n",
    "    print(f\"\\nQuery {i+1}: {result['query']}\")\n",
    "    print(f\"Path length: {result['traversal_path_length']}\")\n",
    "    print(f\"Chunks used: {result['relevant_chunks_count']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
