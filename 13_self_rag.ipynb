{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66d41b8",
   "metadata": {},
   "source": [
    "# Self-RAG：一种动态的 RAG 方法\n",
    "\n",
    "在此笔记本中，我实现了 Self-RAG，这是一个高级 RAG 系统，可动态决定何时以及如何使用检索到的信息。与传统的 RAG 方法不同，Self-RAG 在整个检索和生成过程中引入反射点，从而获得更高的质量和更可靠的响应。\n",
    "\n",
    "## Self-RAG 的关键组件\n",
    "\n",
    "1. 检索决策：确定给定查询是否需要检索\n",
    "2. 文档检索：在需要时获取可能相关的文档\n",
    "3. 相关性评估：评估每个检索到的文档的相关性\n",
    "4. 响应生成：根据相关上下文创建响应\n",
    "5. 支持评估：评估响应是否在上下文中正确接地\n",
    "6. 效用评估：对生成的响应的整体有用性进行评级"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68d36f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz\n",
    "from openai import OpenAI\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b69c0aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Get the page\n",
    "        text = page.get_text(\"text\")  # Extract text from the page\n",
    "        all_text += text  # Append the extracted text to the all_text string\n",
    "\n",
    "    return all_text  # Return the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f4b7111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38af6360",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://api.siliconflow.cn/v1/\",\n",
    "    api_key=os.getenv(\"SILLICONFLOW_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0f99c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the vector store.\n",
    "        \"\"\"\n",
    "        self.vectors = []  # List to store embedding vectors\n",
    "        self.texts = []  # List to store original texts\n",
    "        self.metadata = []  # List to store metadata for each text\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "\n",
    "        Args:\n",
    "        text (str): The original text.\n",
    "        embedding (List[float]): The embedding vector.\n",
    "        metadata (dict, optional): Additional metadata.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n",
    "        self.texts.append(text)  # Add the original text to texts list\n",
    "        self.metadata.append(metadata or {})  # Add metadata to metadata list, default to empty dict if None\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): Query embedding vector.\n",
    "        k (int): Number of results to return.\n",
    "        filter_func (callable, optional): Function to filter results.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: Top k most similar items with their texts and metadata.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return empty list if no vectors are stored\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # Apply filter if provided\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "                \n",
    "            # Calculate cosine similarity\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # Append index and similarity score\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # Add the text\n",
    "                \"metadata\": self.metadata[idx],  # Add the metadata\n",
    "                \"similarity\": score  # Add the similarity score\n",
    "            })\n",
    "        \n",
    "        return results  # Return the list of top k results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d46980ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"BAAI/bge-m3\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text.\n",
    "\n",
    "    Args:\n",
    "    text (str or List[str]): The input text(s) for which embeddings are to be created.\n",
    "    model (str): The model to be used for creating embeddings.\n",
    "\n",
    "    Returns:\n",
    "    List[float] or List[List[float]]: The embedding vector(s).\n",
    "    \"\"\"\n",
    "    # Handle both string and list inputs by converting string input to a list\n",
    "    input_text = text if isinstance(text, list) else [text]\n",
    "    \n",
    "    # Create embeddings for the input text using the specified model\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=input_text\n",
    "    )\n",
    "    \n",
    "    # If the input was a single string, return just the first embedding\n",
    "    if isinstance(text, str):\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    # Otherwise, return all embeddings for the list of texts\n",
    "    return [item.embedding for item in response.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2ed2dbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for Self-RAG.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        chunk_size (int): Size of each chunk in characters.\n",
    "        chunk_overlap (int): Overlap between chunks in characters.\n",
    "\n",
    "    Returns:\n",
    "        SimpleVectorStore: A vector store containing document chunks and their embeddings.\n",
    "    \"\"\"\n",
    "    # Extract text from the PDF file\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Chunk the extracted text\n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    # Create embeddings for each chunk\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # Initialize the vector store\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # Add each chunk and its embedding to the vector store\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df37f0",
   "metadata": {},
   "source": [
    "# Self-RAG Components\n",
    "\n",
    "## 1. Retrieval Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e891e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_if_retrieval_needed(query):\n",
    "    \"\"\"\n",
    "    Determines if retrieval is necessary for the given query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if retrieval is needed, False otherwise\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the AI on how to determine if retrieval is necessary\n",
    "    system_prompt = \"\"\"You are an AI assistant that determines if retrieval is necessary to answer a query.\n",
    "    For factual questions, specific information requests, \n",
    "    or questions about events, people, or concepts, answer \"Yes\".\n",
    "    For opinions, hypothetical scenarios, or simple queries with common knowledge, answer \"No\".\n",
    "    Answer with ONLY \"Yes\" or \"No\".\"\"\"\n",
    "\n",
    "    # User prompt containing the query\n",
    "    user_prompt = f\"Query: {query}\\n\\nIs retrieval necessary to answer this query accurately?\"\n",
    "    \n",
    "    # Generate response from the model\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Qwen/Qwen3-14B\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract the answer from the model's response and convert to lowercase\n",
    "    answer = response.choices[0].message.content.strip().lower()\n",
    "    \n",
    "    # Return True if the answer contains \"yes\", otherwise return False\n",
    "    return \"yes\" in answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e296ea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_relevance(query, context):\n",
    "    \"\"\"\n",
    "    Evaluates the relevance of a context to the query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): Context text\n",
    "        \n",
    "    Returns:\n",
    "        str: 'relevant' or 'irrelevant'\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the AI on how to determine document relevance\n",
    "    system_prompt = \"\"\"You are an AI assistant that determines if a document is relevant to a query.\n",
    "    Consider whether the document contains information that would be helpful in answering the query.\n",
    "    Answer with ONLY \"Relevant\" or \"Irrelevant\".\"\"\"\n",
    "\n",
    "    # Truncate context if it is too long to avoid exceeding token limits\n",
    "    max_context_length = 2000\n",
    "    if len(context) > max_context_length:\n",
    "        context = context[:max_context_length] + \"... [truncated]\"\n",
    "\n",
    "    # User prompt containing the query and the document content\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "    Document content:\n",
    "    {context}\n",
    "\n",
    "    Is this document relevant to the query? Answer with ONLY \"Relevant\" or \"Irrelevant\".\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate response from the model\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Qwen/Qwen3-14B\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract the answer from the model's response and convert to lowercase\n",
    "    answer = response.choices[0].message.content.strip().lower()\n",
    "    \n",
    "    return answer  # Return the relevance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "114b5361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_support(response, context):\n",
    "    system_prompt = \"\"\"You are an AI assistant that determines if a response is supported by the given context.\n",
    "    Evaluate if the facts, claims, and information in the response are backed by the context.\n",
    "    Answer with ONLY one of these three options:\n",
    "    - \"Fully supported\": All information in the response is directly supported by the context.\n",
    "    - \"Partially supported\": Some information in the response is supported by the context, but some is not.\n",
    "    - \"No support\": The response contains significant information not found in or contradicting the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Truncate context if it is too long to avoid exceeding token limits\n",
    "    max_context_length = 2000\n",
    "    if len(context) > max_context_length:\n",
    "        context = context[:max_context_length] + \"... [truncated]\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Context: \n",
    "    {context}\n",
    "    \n",
    "    Response: \n",
    "    {response}\n",
    "    How well is this response supported by the context? \n",
    "    Answer with ONLY \"Fully supported\", \n",
    "    \"Partially supported\", or \"No support\".\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Qwen/Qwen3-14B\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract the answer from the model's response and convert to lowercase\n",
    "    answer = response.choices[0].message.content.strip().lower()\n",
    "    \n",
    "    return answer  # Return the answer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "02d1df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_utility(query, response):\n",
    "    \"\"\"\n",
    "    Rates the utility of a response for the query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        response (str): Generated response\n",
    "        \n",
    "    Returns:\n",
    "        int: Utility rating from 1 to 5\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the AI on how to rate the utility of the response\n",
    "    system_prompt = \"\"\"You are an AI assistant that rates the utility of a response to a query.\n",
    "    Consider how well the response answers the query, its completeness, correctness, and helpfulness.\n",
    "    Rate the utility on a scale from 1 to 5, where:\n",
    "    - 1: Not useful at all\n",
    "    - 2: Slightly useful\n",
    "    - 3: Moderately useful\n",
    "    - 4: Very useful\n",
    "    - 5: Exceptionally useful\n",
    "    Answer with ONLY a single number from 1 to 5.\"\"\"\n",
    "\n",
    "    # User prompt containing the query and the response to be rated\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "    Response:\n",
    "    {response}\n",
    "\n",
    "    Rate the utility of this response on a scale from 1 to 5:\"\"\"\n",
    "    \n",
    "    # Generate the utility rating using the OpenAI client\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Qwen/Qwen3-14B\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract the rating from the model's response\n",
    "    rating = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Extract just the number from the rating\n",
    "    rating_match = re.search(r'[1-5]', rating)\n",
    "    if rating_match:\n",
    "        return int(rating_match.group())  # Return the extracted rating as an integer\n",
    "    \n",
    "    return 3  # Default to middle rating if parsing fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72616d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context=None):\n",
    "    \"\"\"\n",
    "    Generates a response based on the query and optional context.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str, optional): Context text\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the AI on how to generate a helpful response\n",
    "    system_prompt = \"\"\"You are a helpful AI assistant. \n",
    "    Provide a clear, accurate, and informative response to the query.\"\"\"\n",
    "    \n",
    "    # Create the user prompt based on whether context is provided\n",
    "    if context:\n",
    "        user_prompt = f\"\"\"Context:\n",
    "        {context}\n",
    "\n",
    "        Query: {query}\n",
    "\n",
    "        Please answer the query based on the provided context.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        user_prompt = f\"\"\"Query: {query}\n",
    "        \n",
    "        Please answer the query to the best of your ability.\"\"\"\n",
    "    \n",
    "    # Generate the response using the OpenAI client\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Qwen/Qwen3-14B\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    # Return the generated response text\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a9952092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_rag(query, vector_store, top_k=5):\n",
    "    print(f\"\\n=== Starting Self-RAG for query: {query} ===\\n\")\n",
    "    \n",
    "    print(\"Step 1: Determining if retrieval is necessary...\")\n",
    "    retrieval_needed = determine_if_retrieval_needed(query)\n",
    "    print(f\"Retrieval needed: {retrieval_needed}\")\n",
    "    \n",
    "    metrics = {\n",
    "        \"retrieval_needed\": retrieval_needed,\n",
    "        \"documents_retrieved\": 0,\n",
    "        \"relevant_documents\": 0,\n",
    "        \"response_support_ratings\": [],\n",
    "        \"utility_ratings\": [],\n",
    "    }\n",
    "    \n",
    "    best_response = None\n",
    "    best_score = -1\n",
    "    if retrieval_needed:\n",
    "        print(\"\\nStep 2: Retrieving relevant documents...\")\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "        metrics[\"documents_retrieved\"] = len(results)\n",
    "        print(f\"Retrieved {len(results)} documents.\")\n",
    "        \n",
    "        print(\"\\nStep 3: Evaluating document relevance...\")\n",
    "        relevant_contexts = []\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            context = result[\"text\"]\n",
    "            relevance = evaluate_relevance(query, context)\n",
    "            print(f\"Document {i+1}: Relevance = {relevance}\")\n",
    "            \n",
    "            if relevance == \"relevant\":\n",
    "                relevant_contexts.append(context)\n",
    "        \n",
    "        metrics[\"relevant_documents\"] = len(relevant_contexts)\n",
    "        print(f\"Found {len(relevant_contexts)} relevant documents.\")\n",
    "            \n",
    "        if relevant_contexts:\n",
    "            print(\"\\nStep 4: Generating response...\")\n",
    "            for i, context in enumerate(relevant_contexts):\n",
    "                print(f\"\\nProcessing context {i+1}/{len(relevant_contexts)}...\")\n",
    "                \n",
    "                print(\"Generating response...\")\n",
    "                response = generate_response(query, context)\n",
    "                \n",
    "                print(\"Assessing response quality...\")\n",
    "                support_rating = assess_support(query, response)\n",
    "                print(f\"Support rating: {support_rating}\")\n",
    "                metrics[\"response_support_ratings\"].append(support_rating)\n",
    "                \n",
    "                utility_rating = rate_utility(query, response)\n",
    "                print(f\"Utility rating: {utility_rating}/5\")\n",
    "                metrics[\"utility_ratings\"].append(utility_rating)\n",
    "                \n",
    "                support_score ={\n",
    "                    \"fully supported\": 3,\n",
    "                    \"partially supported\": 1,\n",
    "                    \"not supported\": 0,\n",
    "                }.get(support_rating, 0)\n",
    "                \n",
    "                overall_score = support_score * 5 + utility_rating\n",
    "                print(f\"Overall score: {overall_score}\")\n",
    "                \n",
    "                if overall_score > best_score:\n",
    "                    best_response = response\n",
    "                    best_score = overall_score\n",
    "                    print(\"New best response found!\")\n",
    "        \n",
    "        if not relevant_contexts or best_score <= 0:\n",
    "            print(\"\\nNo suitable context found or poor responses, generating without retrieval...\")\n",
    "            best_response = generate_response(query)\n",
    "    else:\n",
    "        print(\"\\nNo retrieval needed, generating response directly...\")\n",
    "        best_response = generate_response(query)\n",
    "    \n",
    "    print(\"Response:\")\n",
    "    print(best_response)\n",
    "    metrics[\"best_score\"] = best_score\n",
    "    metrics[\"used_retrieval\"] = retrieval_needed and best_score > 0\n",
    "    \n",
    "    print(\"\\n=== Self-RAG completed ===\")\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": best_response,\n",
    "        \"metrics\": metrics,\n",
    "    } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3c3b9548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_self_rag_example():\n",
    "    \"\"\"\n",
    "    Demonstrates the complete Self-RAG system with examples.\n",
    "    \"\"\"\n",
    "    # Process document\n",
    "    pdf_path = \"data/AI_Information.pdf\"  # Path to the PDF document\n",
    "    print(f\"Processing document: {pdf_path}\")\n",
    "    vector_store = process_document(pdf_path)  # Process the document and create a vector store\n",
    "    \n",
    "    # Example 1: Query likely needing retrieval\n",
    "    query1 = \"What are the main ethical concerns in AI development?\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"EXAMPLE 1: {query1}\")\n",
    "    result1 = self_rag(query1, vector_store)  # Run Self-RAG for the first query\n",
    "    print(\"\\nFinal response:\")\n",
    "    print(result1[\"response\"])  # Print the final response for the first query\n",
    "    print(\"\\nMetrics:\")\n",
    "    print(json.dumps(result1[\"metrics\"], indent=2))  # Print the metrics for the first query\n",
    "    \n",
    "    # Example 2: Query likely not needing retrieval\n",
    "    query2 = \"Can you write a short poem about artificial intelligence?\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"EXAMPLE 2: {query2}\")\n",
    "    result2 = self_rag(query2, vector_store)  # Run Self-RAG for the second query\n",
    "    print(\"\\nFinal response:\")\n",
    "    print(result2[\"response\"])  # Print the final response for the second query\n",
    "    print(\"\\nMetrics:\")\n",
    "    print(json.dumps(result2[\"metrics\"], indent=2))  # Print the metrics for the second query\n",
    "    \n",
    "    # Example 3: Query with some relevance to document but requiring additional knowledge\n",
    "    query3 = \"How might AI impact economic growth in developing countries?\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"EXAMPLE 3: {query3}\")\n",
    "    result3 = self_rag(query3, vector_store)  # Run Self-RAG for the third query\n",
    "    print(\"\\nFinal response:\")\n",
    "    print(result3[\"response\"])  # Print the final response for the third query\n",
    "    print(\"\\nMetrics:\")\n",
    "    print(json.dumps(result3[\"metrics\"], indent=2))  # Print the metrics for the third query\n",
    "    \n",
    "    return {\n",
    "        \"example1\": result1,\n",
    "        \"example2\": result2,\n",
    "        \"example3\": result3\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e9cc1933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traditional_rag(query, vector_store, top_k=3):\n",
    "    \"\"\"\n",
    "    Implements a traditional RAG approach for comparison.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store containing document chunks\n",
    "        top_k (int): Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Running traditional RAG for query: {query} ===\\n\")\n",
    "    \n",
    "    # Retrieve documents\n",
    "    print(\"Retrieving documents...\")\n",
    "    query_embedding = create_embeddings(query)  # Create embeddings for the query\n",
    "    results = vector_store.similarity_search(query_embedding, k=top_k)  # Search for similar documents\n",
    "    print(f\"Retrieved {len(results)} documents\")\n",
    "    \n",
    "    # Combine contexts from retrieved documents\n",
    "    contexts = [result[\"text\"] for result in results]  # Extract text from results\n",
    "    combined_context = \"\\n\\n\".join(contexts)  # Combine texts into a single context\n",
    "    \n",
    "    # Generate response using the combined context\n",
    "    print(\"Generating response...\")\n",
    "    response = generate_response(query, combined_context)  # Generate response based on the combined context\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "742939d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_approaches(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    Compare Self-RAG with traditional RAG.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the document\n",
    "        test_queries (List[str]): List of test queries\n",
    "        reference_answers (List[str], optional): Reference answers for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation results\n",
    "    \"\"\"\n",
    "    print(\"=== Evaluating RAG Approaches ===\")\n",
    "    \n",
    "    # Process document to create a vector store\n",
    "    vector_store = process_document(pdf_path)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\nProcessing query {i+1}: {query}\")\n",
    "        \n",
    "        # Run Self-RAG\n",
    "        self_rag_result = self_rag(query, vector_store)  # Get response from Self-RAG\n",
    "        self_rag_response = self_rag_result[\"response\"]\n",
    "        \n",
    "        # Run traditional RAG\n",
    "        trad_rag_response = traditional_rag(query, vector_store)  # Get response from traditional RAG\n",
    "        \n",
    "        # Compare results if reference answer is available\n",
    "        reference = reference_answers[i] if reference_answers and i < len(reference_answers) else None\n",
    "        comparison = compare_responses(query, self_rag_response, trad_rag_response, reference)  # Compare responses\n",
    "        \n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"self_rag_response\": self_rag_response,\n",
    "            \"traditional_rag_response\": trad_rag_response,\n",
    "            \"reference_answer\": reference,\n",
    "            \"comparison\": comparison,\n",
    "            \"self_rag_metrics\": self_rag_result[\"metrics\"]\n",
    "        })\n",
    "    \n",
    "    # Generate overall analysis\n",
    "    overall_analysis = generate_overall_analysis(results)\n",
    "    \n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"overall_analysis\": overall_analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "57ee5f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(query, self_rag_response, trad_rag_response, reference=None):\n",
    "    \"\"\"\n",
    "    Compare responses from Self-RAG and traditional RAG.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        self_rag_response (str): Response from Self-RAG\n",
    "        trad_rag_response (str): Response from traditional RAG\n",
    "        reference (str, optional): Reference answer\n",
    "        \n",
    "    Returns:\n",
    "        str: Comparison analysis\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems.\n",
    "    Your task is to compare responses from two different RAG approaches:\n",
    "    1. Self-RAG: A dynamic approach that decides if retrieval is needed and evaluates information relevance and response quality\n",
    "    2. Traditional RAG: Always retrieves documents and uses them to generate a response\n",
    "\n",
    "    Compare the responses based on:\n",
    "    - Relevance to the query\n",
    "    - Factual correctness\n",
    "    - Completeness and informativeness\n",
    "    - Conciseness and focus\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "\n",
    "Response from Self-RAG:\n",
    "{self_rag_response}\n",
    "\n",
    "Response from Traditional RAG:\n",
    "{trad_rag_response}\n",
    "\"\"\"\n",
    "\n",
    "    if reference:\n",
    "        user_prompt += f\"\"\"\n",
    "Reference Answer (for factual checking):\n",
    "{reference}\n",
    "\"\"\"\n",
    "\n",
    "    user_prompt += \"\"\"\n",
    "Compare these responses and explain which one is better and why.\n",
    "Focus on accuracy, relevance, completeness, and quality.\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Qwen/Qwen3-14B\",  # Using a stronger model for evaluation\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "50c8c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_approaches(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    Compare Self-RAG with traditional RAG.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the document\n",
    "        test_queries (List[str]): List of test queries\n",
    "        reference_answers (List[str], optional): Reference answers for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation results\n",
    "    \"\"\"\n",
    "    print(\"=== Evaluating RAG Approaches ===\")\n",
    "    \n",
    "    # Process document to create a vector store\n",
    "    vector_store = process_document(pdf_path)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\nProcessing query {i+1}: {query}\")\n",
    "        \n",
    "        # Run Self-RAG\n",
    "        self_rag_result = self_rag(query, vector_store)  # Get response from Self-RAG\n",
    "        self_rag_response = self_rag_result[\"response\"]\n",
    "        \n",
    "        # Run traditional RAG\n",
    "        trad_rag_response = traditional_rag(query, vector_store)  # Get response from traditional RAG\n",
    "        \n",
    "        # Compare results if reference answer is available\n",
    "        reference = reference_answers[i] if reference_answers and i < len(reference_answers) else None\n",
    "        comparison = compare_responses(query, self_rag_response, trad_rag_response, reference)  # Compare responses\n",
    "        \n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"self_rag_response\": self_rag_response,\n",
    "            \"traditional_rag_response\": trad_rag_response,\n",
    "            \"reference_answer\": reference,\n",
    "            \"comparison\": comparison,\n",
    "            \"self_rag_metrics\": self_rag_result[\"metrics\"]\n",
    "        })\n",
    "    \n",
    "    # Generate overall analysis\n",
    "    overall_analysis = generate_overall_analysis(results)\n",
    "    \n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"overall_analysis\": overall_analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "630d0071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    Generate an overall analysis of Self-RAG vs traditional RAG.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): Results from evaluate_rag_approaches\n",
    "        \n",
    "    Returns:\n",
    "        str: Overall analysis\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Your task is to provide an overall analysis comparing\n",
    "    Self-RAG and Traditional RAG based on multiple test queries.\n",
    "\n",
    "    Focus your analysis on:\n",
    "    1. When Self-RAG performs better and why\n",
    "    2. When Traditional RAG performs better and why\n",
    "    3. The impact of dynamic retrieval decisions in Self-RAG\n",
    "    4. The value of relevance and support evaluation in Self-RAG\n",
    "    5. Overall recommendations on which approach to use for different types of queries\"\"\"\n",
    "\n",
    "    # Prepare a summary of the individual comparisons\n",
    "    comparisons_summary = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        comparisons_summary += f\"Query {i+1}: {result['query']}\\n\"\n",
    "        comparisons_summary += f\"Self-RAG metrics: Retrieval needed: {result['self_rag_metrics']['retrieval_needed']}, \"\n",
    "        comparisons_summary += f\"Relevant docs: {result['self_rag_metrics']['relevant_documents']}/{result['self_rag_metrics']['documents_retrieved']}\\n\"\n",
    "        comparisons_summary += f\"Comparison summary: {result['comparison'][:200]}...\\n\\n\"\n",
    "\n",
    "        user_prompt = f\"\"\"Based on the following comparison results from {len(results)} test queries, please provide an overall analysis of\n",
    "    Self-RAG versus Traditional RAG:\n",
    "\n",
    "    {comparisons_summary}\n",
    "\n",
    "    Please provide your comprehensive analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Qwen/Qwen3-14B\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "df7bd47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluating RAG Approaches ===\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "\n",
      "Processing query 1: What are the main ethical concerns in AI development?\n",
      "\n",
      "=== Starting Self-RAG for query: What are the main ethical concerns in AI development? ===\n",
      "\n",
      "Step 1: Determining if retrieval is necessary...\n",
      "Retrieval needed: True\n",
      "\n",
      "Step 2: Retrieving relevant documents...\n",
      "Retrieved 5 documents.\n",
      "\n",
      "Step 3: Evaluating document relevance...\n",
      "Document 1: Relevance = relevant\n",
      "Document 2: Relevance = relevant\n",
      "Document 3: Relevance = relevant\n",
      "Document 4: Relevance = relevant\n",
      "Document 5: Relevance = relevant\n",
      "Found 5 relevant documents.\n",
      "\n",
      "Step 4: Generating response...\n",
      "\n",
      "Processing context 1/5...\n",
      "Generating response...\n",
      "Assessing response quality...\n",
      "Support rating: fully supported\n",
      "Utility rating: 5/5\n",
      "Overall score: 20\n",
      "New best response found!\n",
      "\n",
      "Processing context 2/5...\n",
      "Generating response...\n",
      "Assessing response quality...\n",
      "Support rating: no support\n",
      "Utility rating: 5/5\n",
      "Overall score: 5\n",
      "\n",
      "Processing context 3/5...\n",
      "Generating response...\n",
      "Assessing response quality...\n",
      "Support rating: no support\n",
      "Utility rating: 3/5\n",
      "Overall score: 3\n",
      "\n",
      "Processing context 4/5...\n",
      "Generating response...\n",
      "Assessing response quality...\n",
      "Support rating: no support\n",
      "Utility rating: 4/5\n",
      "Overall score: 4\n",
      "\n",
      "Processing context 5/5...\n",
      "Generating response...\n",
      "Assessing response quality...\n",
      "Support rating: no support\n",
      "Utility rating: 4/5\n",
      "Overall score: 4\n",
      "Response:\n",
      "The main ethical concerns in AI development, as outlined in the provided context, include:  \n",
      "\n",
      "1. **Bias**: AI systems may inherit or amplify biases present in training data, leading to unfair or discriminatory outcomes.  \n",
      "2. **Transparency**: Ensuring AI systems are explainable and their decision-making processes are understandable to users and stakeholders.  \n",
      "3. **Privacy**: Protecting personal data and ensuring AI systems do not infringe on individuals' privacy rights.  \n",
      "4. **Safety**: Guaranteeing that AI systems operate reliably and do not cause harm, whether through errors, unintended consequences, or misuse.  \n",
      "\n",
      "Balancing innovation with these ethical considerations is highlighted as a key challenge in AI development. Additionally, the context emphasizes the importance of public engagement, education, and international cooperation to address these concerns responsibly.\n",
      "\n",
      "=== Self-RAG completed ===\n",
      "\n",
      "=== Running traditional RAG for query: What are the main ethical concerns in AI development? ===\n",
      "\n",
      "Retrieving documents...\n",
      "Retrieved 3 documents\n",
      "Generating response...\n",
      "\n",
      "=== OVERALL ANALYSIS ===\n",
      "\n",
      "\n",
      "\n",
      "### **Comprehensive Analysis of Self-RAG vs. Traditional RAG**  \n",
      "Based on the test query **\"What are the main ethical concerns in AI development?\"** and the observed performance, here is a detailed analysis of the two approaches:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. When Self-RAG Performs Better and Why**  \n",
      "**Self-RAG excels in scenarios where:**  \n",
      "- **Dynamic retrieval decisions are critical**: For complex, multi-step queries that require iterative information gathering (e.g., \"How does AI bias affect healthcare outcomes, and what are the solutions?\"), Self-RAG’s ability to decide when to retrieve additional documents can lead to more nuanced and accurate responses.  \n",
      "- **Contextual relevance is ambiguous**: If the initial retrieval step misses key documents, Self-RAG’s self-evaluation of relevance and support can trigger a second retrieval phase to fill gaps.  \n",
      "- **The query requires balancing multiple perspectives**: Self-RAG’s evaluation of relevance and support ensures that the response incorporates diverse viewpoints (e.g., ethical, technical, societal) without over-reliance on a static corpus.  \n",
      "\n",
      "**Why it might not perform well here**:  \n",
      "In this test case, the query is **well-defined and fact-based**, with a clear set of ethical concerns (e.g., bias, transparency, accountability). Traditional RAG’s static retrieval likely captured all necessary documents in one pass, while Self-RAG’s dynamic decisions may have introduced unnecessary complexity or delays, leading to a less polished response.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. When Traditional RAG Performs Better and Why**  \n",
      "**Traditional RAG is superior for:**  \n",
      "- **Straightforward, fact-based queries**: When the answer is directly available in a static corpus (e.g., \"What are the main ethical concerns in AI development?\"), Traditional RAG avoids the overhead of dynamic retrieval decisions and produces a concise, accurate response.  \n",
      "- **Highly structured domains**: In fields with well-documented knowledge (e.g., medicine, law), static retrieval ensures consistency and avoids the risk of the model misjudging relevance during dynamic steps.  \n",
      "- **Scenarios with limited computational resources**: Traditional RAG’s fixed retrieval process is computationally lighter, making it more efficient for simple queries.  \n",
      "\n",
      "**Why it outperformed Self-RAG here**:  \n",
      "The query’s simplicity and the availability of comprehensive documents in the corpus meant that Traditional RAG’s static retrieval was sufficient. Self-RAG’s additional steps (e.g., evaluating relevance, deciding to retrieve) may have introduced redundancy or subtle errors in the response, even though it retrieved all 5 relevant documents.\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Impact of Dynamic Retrieval Decisions in Self-RAG**  \n",
      "- **Pros**:  \n",
      "  - Enables the model to adapt to ambiguous or evolving queries (e.g., \"What are the latest ethical guidelines for AI in finance?\").  \n",
      "  - Reduces reliance on a static corpus by allowing iterative refinement of the answer.  \n",
      "- **Cons in this case**:  \n",
      "  - For well-defined queries, dynamic decisions may be **overkill**, leading to unnecessary computational overhead or potential misjudgments (e.g., the model might incorrectly decide not to retrieve additional documents).  \n",
      "  - The evaluation of relevance and support during dynamic steps can be error-prone if the model lacks fine-tuning for specific domains.  \n",
      "\n",
      "---\n",
      "\n",
      "### **4. Value of Relevance and Support Evaluation in Self-RAG**  \n",
      "- **Strengths**:  \n",
      "  - Ensures the response is **factually grounded** and avoids hallucinations by verifying that retrieved documents directly support the answer.  \n",
      "  - Useful for complex queries where the model must distinguish between tangential and critical information.  \n",
      "- **Limitations in this case**:  \n",
      "  - The evaluation process may have **slowed down the response** or introduced minor inaccuracies (e.g., the model might have prioritized less relevant documents due to imperfect scoring).  \n",
      "  - For straightforward queries, this evaluation adds **unnecessary complexity** without clear benefits.  \n",
      "\n",
      "---\n",
      "\n",
      "### **5. Overall Recommendations**  \n",
      "| **Query Type**               | **Recommended Approach** | **Rationale**                                                                 |\n",
      "|-----------------------------|--------------------------|-------------------------------------------------------------------------------|\n",
      "| **Simple, fact-based**      | **Traditional RAG**      | Static retrieval is efficient and avoids overhead from dynamic decisions.     |\n",
      "| **Complex, multi-step**     | **Self-RAG**             | Dynamic retrieval ensures comprehensive coverage of nuanced or evolving topics. |\n",
      "| **Ambiguous or open-ended** | **Self-RAG**             | Self-evaluation of relevance and support helps navigate uncertainty.          |\n",
      "| **Highly structured domains**| **Traditional RAG**      | Static retrieval ensures consistency and avoids errors in dynamic steps.      |\n",
      "\n",
      "---\n",
      "\n",
      "### **Conclusion**  \n",
      "While **Self-RAG** offers flexibility and robustness for complex queries, **Traditional RAG** remains superior for straightforward, well-defined questions like the one tested. The key takeaway is that **the choice between the two depends on the query’s complexity and the trade-off between accuracy and efficiency**. For the given test case, Traditional RAG’s simplicity and reliability made it the better choice, but Self-RAG’s dynamic capabilities are invaluable in more challenging scenarios.\n"
     ]
    }
   ],
   "source": [
    "# Path to the AI information document\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# Define test queries covering different query types to test Self-RAG's adaptive retrieval\n",
    "test_queries = [\n",
    "    \"What are the main ethical concerns in AI development?\",        # Document-focused query\n",
    "    # \"How does explainable AI improve trust in AI systems?\",         # Document-focused query\n",
    "    # \"Write a poem about artificial intelligence\",                   # Creative query, doesn't need retrieval\n",
    "    # \"Will superintelligent AI lead to human obsolescence?\"          # Speculative query, partial retrieval needed\n",
    "]\n",
    "\n",
    "# Reference answers for more objective evaluation\n",
    "reference_answers = [\n",
    "    \"The main ethical concerns in AI development include bias and fairness, privacy, transparency, accountability, safety, and the potential for misuse or harmful applications.\",\n",
    "    # \"Explainable AI improves trust by making AI decision-making processes transparent and understandable to users, helping them verify fairness, identify potential biases, and better understand AI limitations.\",\n",
    "    # \"A quality poem about artificial intelligence should creatively explore themes of AI's capabilities, limitations, relationship with humanity, potential futures, or philosophical questions about consciousness and intelligence.\",\n",
    "    # \"Views on superintelligent AI's impact on human relevance vary widely. Some experts warn of potential risks if AI surpasses human capabilities across domains, possibly leading to economic displacement or loss of human agency. Others argue humans will remain relevant through complementary skills, emotional intelligence, and by defining AI's purpose. Most experts agree that thoughtful governance and human-centered design are essential regardless of the outcome.\"\n",
    "]\n",
    "\n",
    "# Run the evaluation comparing Self-RAG with traditional RAG approaches\n",
    "evaluation_results = evaluate_rag_approaches(\n",
    "    pdf_path=pdf_path,                  # Source document containing AI information\n",
    "    test_queries=test_queries,          # List of AI-related test queries\n",
    "    reference_answers=reference_answers  # Ground truth answers for evaluation\n",
    ")\n",
    "\n",
    "# Print the overall comparative analysis\n",
    "print(\"\\n=== OVERALL ANALYSIS ===\\n\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
