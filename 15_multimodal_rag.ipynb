{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 带有图像字幕的多模式RAG\n",
    "\n",
    "在此笔记本中，我实现了一个多模式的RAG系统，该系统从文档中提取文本和图像，为图像生成字幕，并使用两种内容类型来响应查询。这种方法通过将视觉信息纳入知识库来增强传统RAG。\n",
    "\n",
    "传统的RAG系统仅与文本一起使用，但是许多文档都包含图像，图表和表格中的重要信息。通过将这些视觉元素加上标题并将其纳入我们的检索系统，我们可以：\n",
    "\n",
    " - 锁定在数字和图表中的访问信息\n",
    " - 了解补充文本的表和图表\n",
    " - 创建更全面的知识库\n",
    " - 回答依赖视觉数据的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz\n",
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "import base64\n",
    "import re\n",
    "import tempfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.siliconflow.cn/v1/\",\n",
    "    api_key=os.getenv(\"SILLICONFLOW_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_from_pdf(pdf_path, output_dir=None):\n",
    "    \"\"\"\n",
    "    Extract both text and images from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        output_dir (str, optional): Directory to save extracted images\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[Dict], List[Dict]]: Text data and image data\n",
    "    \"\"\"\n",
    "    # Create a temporary directory for images if not provided\n",
    "    temp_dir = None\n",
    "    if output_dir is None:\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        output_dir = temp_dir\n",
    "    else:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "    text_data = []  # List to store extracted text data\n",
    "    image_paths = []  # List to store paths of extracted images\n",
    "    \n",
    "    print(f\"Extracting content from {pdf_path}...\")\n",
    "    \n",
    "    try:\n",
    "        with fitz.open(pdf_path) as pdf_file:\n",
    "            # Loop through every page in the PDF\n",
    "            for page_number in range(len(pdf_file)):\n",
    "                page = pdf_file[page_number]\n",
    "                \n",
    "                # Extract text from the page\n",
    "                text = page.get_text().strip()\n",
    "                if text:\n",
    "                    text_data.append({\n",
    "                        \"content\": text,\n",
    "                        \"metadata\": {\n",
    "                            \"source\": pdf_path,\n",
    "                            \"page\": page_number + 1,\n",
    "                            \"type\": \"text\"\n",
    "                        }\n",
    "                    })\n",
    "                \n",
    "                # Extract images from the page\n",
    "                image_list = page.get_images(full=True)\n",
    "                for img_index, img in enumerate(image_list):\n",
    "                    xref = img[0]  # XREF of the image\n",
    "                    base_image = pdf_file.extract_image(xref)\n",
    "                    \n",
    "                    if base_image:\n",
    "                        image_bytes = base_image[\"image\"]\n",
    "                        image_ext = base_image[\"ext\"]\n",
    "                        \n",
    "                        # Save the image to the output directory\n",
    "                        img_filename = f\"page_{page_number+1}_img_{img_index+1}.{image_ext}\"\n",
    "                        img_path = os.path.join(output_dir, img_filename)\n",
    "                        \n",
    "                        with open(img_path, \"wb\") as img_file:\n",
    "                            img_file.write(image_bytes)\n",
    "                        \n",
    "                        image_paths.append({\n",
    "                            \"path\": img_path,\n",
    "                            \"metadata\": {\n",
    "                                \"source\": pdf_path,\n",
    "                                \"page\": page_number + 1,\n",
    "                                \"image_index\": img_index + 1,\n",
    "                                \"type\": \"image\"\n",
    "                            }\n",
    "                        })\n",
    "        \n",
    "        print(f\"Extracted {len(text_data)} text segments and {len(image_paths)} images\")\n",
    "        return text_data, image_paths\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting content: {e}\")\n",
    "        if temp_dir and os.path.exists(temp_dir):\n",
    "            shutil.rmtree(temp_dir)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking Text Content\n",
    "def chunk_text(text_data, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Split text data into overlapping chunks.\n",
    "    \n",
    "    Args:\n",
    "        text_data (List[Dict]): Text data extracted from PDF\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Chunked text data\n",
    "    \"\"\"\n",
    "    chunked_data = []  # Initialize an empty list to store chunked data\n",
    "    \n",
    "    for item in text_data:\n",
    "        text = item[\"content\"]  # Extract the text content\n",
    "        metadata = item[\"metadata\"]  # Extract the metadata\n",
    "        \n",
    "        # Skip if text is too short\n",
    "        if len(text) < chunk_size / 2:\n",
    "            chunked_data.append({\n",
    "                \"content\": text,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Create chunks with overlap\n",
    "        chunks = []\n",
    "        for i in range(0, len(text), chunk_size - overlap):\n",
    "            chunk = text[i:i + chunk_size]  # Extract a chunk of the specified size\n",
    "            if chunk:  # Ensure we don't add empty chunks\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        # Add each chunk with updated metadata\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_metadata = metadata.copy()  # Copy the original metadata\n",
    "            chunk_metadata[\"chunk_index\"] = i  # Add chunk index to metadata\n",
    "            chunk_metadata[\"chunk_count\"] = len(chunks)  # Add total chunk count to metadata\n",
    "            \n",
    "            chunked_data.append({\n",
    "                \"content\": chunk,  # The chunk text\n",
    "                \"metadata\": chunk_metadata  # The updated metadata\n",
    "            })\n",
    "    \n",
    "    print(f\"Created {len(chunked_data)} text chunks\")  # Print the number of created chunks\n",
    "    return chunked_data  # Return the list of chunked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"\n",
    "    Encode an image file as base64.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        str: Base64 encoded image\n",
    "    \"\"\"\n",
    "    # Open the image file in binary read mode\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        # Read the image file and encode it to base64\n",
    "        encoded_image = base64.b64encode(image_file.read())\n",
    "        # Decode the base64 bytes to a string and return\n",
    "        return encoded_image.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_caption(image_path):\n",
    "    \"\"\"\n",
    "    Generate a caption for an image using OpenAI's vision capabilities.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated caption\n",
    "    \"\"\"\n",
    "    # Check if the file exists and is an image\n",
    "    if not os.path.exists(image_path):\n",
    "        return \"Error: Image file not found\"\n",
    "    \n",
    "    try:\n",
    "        # Open and validate the image\n",
    "        Image.open(image_path)\n",
    "        print(\"image_path:\"+image_path)\n",
    "        # Encode the image to base64\n",
    "        base64_image = encode_image(image_path)\n",
    "        \n",
    "        # Create the API request to generate the caption\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"Qwen/Qwen2.5-VL-32B-Instruct\", # Use the llava-1.5-7b model\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"You are an assistant specialized in describing images from academic papers. \n",
    "                    Provide detailed captions for the image that capture key information. \n",
    "                    If the image contains charts, tables, or diagrams, describe their content and purpose clearly. \n",
    "                    Your caption should be optimized for future retrieval when people ask questions about this content.\"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Describe this image in detail, focusing on its academic content:\"},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        # Extract the caption from the response\n",
    "        caption = response.choices[0].message.content\n",
    "        return caption\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Return an error message if an exception occurs\n",
    "        return f\"Error generating caption: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(image_paths):\n",
    "    \"\"\"\n",
    "    Process all images and generate captions.\n",
    "    \n",
    "    Args:\n",
    "        image_paths (List[Dict]): Paths to extracted images\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Image data with captions\n",
    "    \"\"\"\n",
    "    image_data = []  # Initialize an empty list to store image data with captions\n",
    "    \n",
    "    print(f\"Generating captions for {len(image_paths)} images...\")  # Print the number of images to process\n",
    "    for i, img_item in enumerate(image_paths):\n",
    "        print(f\"Processing image {i+1}/{len(image_paths)}...\")  # Print the current image being processed\n",
    "        img_path = img_item[\"path\"]  # Get the image path\n",
    "        metadata = img_item[\"metadata\"]  # Get the image metadata\n",
    "        \n",
    "        # Generate caption for the image\n",
    "        caption = generate_image_caption(img_path)\n",
    "        print(\"Caption: \"+caption)\n",
    "        # Add the image data with caption to the list\n",
    "        image_data.append({\n",
    "            \"content\": caption,  # The generated caption\n",
    "            \"metadata\": metadata,  # The image metadata\n",
    "            \"image_path\": img_path  # The path to the image\n",
    "        })\n",
    "    \n",
    "    return image_data  # Return the list of image data with captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation for multi-modal content.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Initialize lists to store vectors, contents, and metadata\n",
    "        self.vectors = []\n",
    "        self.contents = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, content, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            content (str): The content (text or image caption)\n",
    "            embedding (List[float]): The embedding vector\n",
    "            metadata (Dict, optional): Additional metadata\n",
    "        \"\"\"\n",
    "        # Append the embedding vector, content, and metadata to their respective lists\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.contents.append(content)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def add_items(self, items, embeddings):\n",
    "        \"\"\"\n",
    "        Add multiple items to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            items (List[Dict]): List of content items\n",
    "            embeddings (List[List[float]]): List of embedding vectors\n",
    "        \"\"\"\n",
    "        # Loop through items and embeddings and add each to the vector store\n",
    "        for item, embedding in zip(items, embeddings):\n",
    "            self.add_item(\n",
    "                content=item[\"content\"],\n",
    "                embedding=embedding,\n",
    "                metadata=item.get(\"metadata\", {})\n",
    "            )\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): Query embedding vector\n",
    "            k (int): Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: Top k most similar items\n",
    "        \"\"\"\n",
    "        # Return an empty list if there are no vectors in the store\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"content\": self.contents[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": float(score)  # Convert to float for JSON serialization\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=\"BAAI/bge-m3\"):\n",
    "    \"\"\"\n",
    "    Create embeddings for the given texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): Input texts\n",
    "        model (str): Embedding model name\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: Embedding vectors\n",
    "    \"\"\"\n",
    "    # Handle empty input\n",
    "    if not texts:\n",
    "        return []\n",
    "        \n",
    "    # Process in batches if needed (OpenAI API limits)\n",
    "    batch_size = 100\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Iterate over the input texts in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]  # Get the current batch of texts\n",
    "        \n",
    "        # Create embeddings for the current batch\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=batch\n",
    "        )\n",
    "        \n",
    "        # Extract embeddings from the response\n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n",
    "    \n",
    "    return all_embeddings  # Return all embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for multi-modal RAG.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[MultiModalVectorStore, Dict]: Vector store and document info\n",
    "    \"\"\"\n",
    "    # Create a directory for extracted images\n",
    "    image_dir = \"extracted_images\"\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract text and images from the PDF\n",
    "    text_data, image_paths = extract_content_from_pdf(pdf_path, image_dir)\n",
    "    \n",
    "    # Chunk the extracted text\n",
    "    chunked_text = chunk_text(text_data, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # Process the extracted images to generate captions\n",
    "    image_data = process_images(image_paths)\n",
    "    \n",
    "    # Combine all content items (text chunks and image captions)\n",
    "    all_items = chunked_text + image_data\n",
    "    \n",
    "    # Extract content for embedding\n",
    "    contents = [item[\"content\"] for item in all_items]\n",
    "    \n",
    "    # Create embeddings for all content\n",
    "    print(\"Creating embeddings for all content...\")\n",
    "    embeddings = create_embeddings(contents)\n",
    "    \n",
    "    # Build the vector store and add items with their embeddings\n",
    "    vector_store = MultiModalVectorStore()\n",
    "    vector_store.add_items(all_items, embeddings)\n",
    "    \n",
    "    # Prepare document info with counts of text chunks and image captions\n",
    "    doc_info = {\n",
    "        \"text_count\": len(chunked_text),\n",
    "        \"image_count\": len(image_data),\n",
    "        \"total_items\": len(all_items),\n",
    "    }\n",
    "    \n",
    "    # Print summary of added items\n",
    "    print(f\"Added {len(all_items)} items to vector store ({len(chunked_text)} text chunks, {len(image_data)} image captions)\")\n",
    "    \n",
    "    # Return the vector store and document info\n",
    "    return vector_store, doc_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, results):\n",
    "    \"\"\"\n",
    "    Generate a response based on the query and retrieved results.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        results (List[Dict]): Retrieved content\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Format the context from the retrieved results\n",
    "    context = \"\"\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        # Determine the type of content (text or image caption)\n",
    "        content_type = \"Text\" if result[\"metadata\"].get(\"type\") == \"text\" else \"Image caption\"\n",
    "        # Get the page number from the metadata\n",
    "        page_num = result[\"metadata\"].get(\"page\", \"unknown\")\n",
    "        \n",
    "        # Append the content type and page number to the context\n",
    "        context += f\"[{content_type} from page {page_num}]\\n\"\n",
    "        # Append the actual content to the context\n",
    "        context += result[\"content\"]\n",
    "        context += \"\\n\\n\"\n",
    "    \n",
    "    # System message to guide the AI assistant\n",
    "    system_message = \"\"\"You are an AI assistant specializing in answering questions about documents \n",
    "    that contain both text and images. You have been given relevant text passages and image captions \n",
    "    from the document. Use this information to provide a comprehensive, accurate response to the query.\n",
    "    If information comes from an image or chart, mention this in your answer.\n",
    "    If the retrieved information doesn't fully answer the query, acknowledge the limitations.\"\"\"\n",
    "\n",
    "    # User message containing the query and the formatted context\n",
    "    user_message = f\"\"\"Query: {query}\n",
    "\n",
    "    Retrieved content:\n",
    "    {context}\n",
    "\n",
    "    Please answer the query based on the retrieved content.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the response using the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Qwen/Qwen3-14B\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    # Return the generated response\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_multimodal_rag(query, vector_store, k=5):\n",
    "    \"\"\"\n",
    "    Query the multi-modal RAG system.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (MultiModalVectorStore): Vector store with document content\n",
    "        k (int): Number of results to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Query results and generated response\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Processing query: {query} ===\\n\")\n",
    "    \n",
    "    # Generate embedding for the query\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # Retrieve relevant content from the vector store\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    \n",
    "    # Separate text and image results\n",
    "    text_results = [r for r in results if r[\"metadata\"].get(\"type\") == \"text\"]\n",
    "    image_results = [r for r in results if r[\"metadata\"].get(\"type\") == \"image\"]\n",
    "    \n",
    "    print(f\"Retrieved {len(results)} relevant items ({len(text_results)} text, {len(image_results)} image captions)\")\n",
    "    \n",
    "    # Generate a response using the retrieved content\n",
    "    response = generate_response(query, results)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"results\": results,\n",
    "        \"response\": response,\n",
    "        \"text_results_count\": len(text_results),\n",
    "        \"image_results_count\": len(image_results)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(query, mm_response, text_response, reference=None):\n",
    "    \"\"\"\n",
    "    Compare multi-modal and text-only responses.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        mm_response (str): Multi-modal response\n",
    "        text_response (str): Text-only response\n",
    "        reference (str, optional): Reference answer\n",
    "        \n",
    "    Returns:\n",
    "        str: Comparison analysis\n",
    "    \"\"\"\n",
    "    # System prompt for the evaluator\n",
    "    system_prompt = \"\"\"You are an expert evaluator comparing two RAG systems:\n",
    "    1. Multi-modal RAG: Retrieves from both text and image captions\n",
    "    2. Text-only RAG: Retrieves only from text\n",
    "\n",
    "    Evaluate which response better answers the query based on:\n",
    "    - Accuracy and correctness\n",
    "    - Completeness of information\n",
    "    - Relevance to the query\n",
    "    - Unique information from visual elements (for multi-modal)\"\"\"\n",
    "\n",
    "    # User prompt with query and responses\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "\n",
    "    Multi-modal RAG Response:\n",
    "    {mm_response}\n",
    "\n",
    "    Text-only RAG Response:\n",
    "    {text_response}\n",
    "    \"\"\"\n",
    "\n",
    "    if reference:\n",
    "        user_prompt += f\"\"\"\n",
    "    Reference Answer:\n",
    "    {reference}\n",
    "    \"\"\"\n",
    "\n",
    "        user_prompt += \"\"\"\n",
    "    Compare these responses and explain which one better answers the query and why.\n",
    "    Note any specific information that came from images in the multi-modal response.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate comparison using meta-llama/Llama-3.2-3B-Instruct\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Qwen/Qwen3-14B\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    Generate an overall analysis of multi-modal vs text-only RAG.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): Evaluation results for each query\n",
    "        \n",
    "    Returns:\n",
    "        str: Overall analysis\n",
    "    \"\"\"\n",
    "    # System prompt for the evaluator\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Provide an overall analysis comparing \n",
    "    multi-modal RAG (text + images) versus text-only RAG based on multiple test queries.\n",
    "\n",
    "    Focus on:\n",
    "    1. Types of queries where multi-modal RAG outperforms text-only\n",
    "    2. Specific advantages of incorporating image information\n",
    "    3. Any disadvantages or limitations of the multi-modal approach\n",
    "    4. Overall recommendation on when to use each approach\"\"\"\n",
    "\n",
    "    # Create summary of evaluations\n",
    "    evaluations_summary = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n",
    "        evaluations_summary += f\"Multi-modal retrieved {result['multimodal_results']['text_count']} text chunks and {result['multimodal_results']['image_count']} image captions\\n\"\n",
    "        evaluations_summary += f\"Comparison summary: {result['comparison'][:200]}...\\n\\n\"\n",
    "\n",
    "    # User prompt with evaluations summary\n",
    "    user_prompt = f\"\"\"Based on the following evaluations of multi-modal vs text-only RAG across {len(results)} queries, \n",
    "    provide an overall analysis comparing these two approaches:\n",
    "\n",
    "    {evaluations_summary}\n",
    "\n",
    "    Please provide a comprehensive analysis of the relative strengths and weaknesses of multi-modal RAG \n",
    "    compared to text-only RAG, with specific attention to how image information contributed (or didn't contribute) to response quality.\"\"\"\n",
    "\n",
    "    # Generate overall analysis using meta-llama/Llama-3.2-3B-Instruct\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Qwen/Qwen3-14B\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_only_store(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Build a text-only vector store for comparison.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        MultiModalVectorStore: Text-only vector store\n",
    "    \"\"\"\n",
    "    # Extract text from PDF (reuse function but ignore images)\n",
    "    text_data, _ = extract_content_from_pdf(pdf_path, None)\n",
    "    \n",
    "    # Chunk text\n",
    "    chunked_text = chunk_text(text_data, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # Extract content for embedding\n",
    "    contents = [item[\"content\"] for item in chunked_text]\n",
    "    \n",
    "    # Create embeddings\n",
    "    print(\"Creating embeddings for text-only content...\")\n",
    "    embeddings = create_embeddings(contents)\n",
    "    \n",
    "    # Build vector store\n",
    "    vector_store = MultiModalVectorStore()\n",
    "    vector_store.add_items(chunked_text, embeddings)\n",
    "    \n",
    "    print(f\"Added {len(chunked_text)} text items to text-only vector store\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multimodal_vs_textonly(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    Compare multi-modal RAG with text-only RAG.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        test_queries (List[str]): Test queries\n",
    "        reference_answers (List[str], optional): Reference answers\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results\n",
    "    \"\"\"\n",
    "    print(\"=== EVALUATING MULTI-MODAL RAG VS TEXT-ONLY RAG ===\\n\")\n",
    "    \n",
    "    # Process document for multi-modal RAG\n",
    "    print(\"\\nProcessing document for multi-modal RAG...\")\n",
    "    mm_vector_store, mm_doc_info = process_document(pdf_path)\n",
    "    \n",
    "    # Build text-only store\n",
    "    print(\"\\nProcessing document for text-only RAG...\")\n",
    "    text_vector_store = build_text_only_store(pdf_path)\n",
    "    \n",
    "    # Run evaluation for each query\n",
    "    results = []\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\n=== Evaluating Query {i+1}: {query} ===\")\n",
    "        \n",
    "        # Get reference answer if available\n",
    "        reference = None\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference = reference_answers[i]\n",
    "        \n",
    "        # Run multi-modal RAG\n",
    "        print(\"\\nRunning multi-modal RAG...\")\n",
    "        mm_result = query_multimodal_rag(query, mm_vector_store)\n",
    "        \n",
    "        # Run text-only RAG\n",
    "        print(\"\\nRunning text-only RAG...\")\n",
    "        text_result = query_multimodal_rag(query, text_vector_store)\n",
    "        \n",
    "        # Compare responses\n",
    "        comparison = compare_responses(query, mm_result[\"response\"], text_result[\"response\"], reference)\n",
    "        \n",
    "        # Add to results\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"multimodal_response\": mm_result[\"response\"],\n",
    "            \"textonly_response\": text_result[\"response\"],\n",
    "            \"multimodal_results\": {\n",
    "                \"text_count\": mm_result[\"text_results_count\"],\n",
    "                \"image_count\": mm_result[\"image_results_count\"]\n",
    "            },\n",
    "            \"reference_answer\": reference,\n",
    "            \"comparison\": comparison\n",
    "        })\n",
    "    \n",
    "    # Generate overall analysis\n",
    "    overall_analysis = generate_overall_analysis(results)\n",
    "    \n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"overall_analysis\": overall_analysis,\n",
    "        \"multimodal_doc_info\": mm_doc_info\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估多模态RAG vs 文本RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUATING MULTI-MODAL RAG VS TEXT-ONLY RAG ===\n",
      "\n",
      "\n",
      "Processing document for multi-modal RAG...\n",
      "Extracting content from data/attention_is_all_you_need.pdf...\n",
      "Extracted 15 text segments and 3 images\n",
      "Created 56 text chunks\n",
      "Generating captions for 3 images...\n",
      "Processing image 1/3...\n",
      "LLM Image Recognized Caption:This image illustrates the architecture of a transformer model. The model is characterized by its stackable layers of identical sub-layers, which include attention mechanisms and feed-forward neural networks. The left side of the image represents the input processing pipeline: Input Embedding is preceded by Positional Encoding and fed into the first layer which comprises Multi-Head Attention followed by an Add & Norm operation, and a Feed Forward layer. The right side follows the same structure as the input, an Output Embedding that is shifted right as well as Positional Encoding is added before passing through similar transformer layers. add & norm blocks provide residual paths connecting the output of one layer to the input of the next. Tailored attention mechanisms, including standard Multi-Head Attention and Masked Multi-Head Attention, attend to sequential dependencies at various points and conditions, and residue connections help in conveying information from skipped layers. The final step uses a Linear layer followed by a Softmax function to produce the output probabilities, analyzing the inputs and outputs across tokens with varied capabilities of downside.\n",
      "Caption:This image illustrates the architecture of a transformer model. The model is characterized by its stackable layers of identical sub-layers, which include attention mechanisms and feed-forward neural networks. The left side of the image represents the input processing pipeline: Input Embedding is preceded by Positional Encoding and fed into the first layer which comprises Multi-Head Attention followed by an Add & Norm operation, and a Feed Forward layer. The right side follows the same structure as the input, an Output Embedding that is shifted right as well as Positional Encoding is added before passing through similar transformer layers. add & norm blocks provide residual paths connecting the output of one layer to the input of the next. Tailored attention mechanisms, including standard Multi-Head Attention and Masked Multi-Head Attention, attend to sequential dependencies at various points and conditions, and residue connections help in conveying information from skipped layers. The final step uses a Linear layer followed by a Softmax function to produce the output probabilities, analyzing the inputs and outputs across tokens with varied capabilities of downside.\n",
      "Processing image 2/3...\n",
      "LLM Image Recognized Caption:This image represents a simplified diagram of the Multi-Head Self-Attention mechanism used in transformer models. The diagram illustrates a single head of the self-attention process, where input queries (Q), keys (K), and values (V) are transformed through a series of operations.\n",
      "\n",
      "1. The input还有个 bloom filter，一但拿 prayer bloom filter 而且捞出那些值就great 那 just拿这个 各个子项性和标签一个个 merge 之前，把一些交其他的然后然后 Stern Lee Powerful  See informationcharacterinfo\n",
      "Caption:This image represents a simplified diagram of the Multi-Head Self-Attention mechanism used in transformer models. The diagram illustrates a single head of the self-attention process, where input queries (Q), keys (K), and values (V) are transformed through a series of operations.\n",
      "\n",
      "1. The input还有个 bloom filter，一但拿 prayer bloom filter 而且捞出那些值就great 那 just拿这个 各个子项性和标签一个个 merge 之前，把一些交其他的然后然后 Stern Lee Powerful  See informationcharacterinfo\n",
      "Processing image 3/3...\n",
      "LLM Image Recognized Caption:This image depicts a schematic of a multi-head scaled dot-product attention mechanism, a key component in transformer models. The structure illustrates how the inputs \\(V\\), \\(K\\), and \\(Q\\) (representing values, keys, and queries respectively) are processed through linear transformations to produce the output \\(h\\). The attention mechanism is denoted by the labeled block \"Scaled Dot-Product Attention,\" which computes the attention weights by taking the dot product of the queries and keys scaled-by-the-square-root of the key dimension, followed by a softmax function that normalizes these weights. The visual representation shows multiple parallel pathways (indicated by the重复的线性层和点积注意力块的堆叠), suggesting the multi-head aspect where each pathway processes the inputs independently and combines the results to form the final output \\(h\\). The \"Concat\" block at the top combines the outputs from all heads before passing the results through another linear layer to produce the final output. This design reflects how transformers efficiently handle dependencies through self-attention mechanisms, which are crucial for tasks involving sequential data like natural language processing.\n",
      "Caption:This image depicts a schematic of a multi-head scaled dot-product attention mechanism, a key component in transformer models. The structure illustrates how the inputs \\(V\\), \\(K\\), and \\(Q\\) (representing values, keys, and queries respectively) are processed through linear transformations to produce the output \\(h\\). The attention mechanism is denoted by the labeled block \"Scaled Dot-Product Attention,\" which computes the attention weights by taking the dot product of the queries and keys scaled-by-the-square-root of the key dimension, followed by a softmax function that normalizes these weights. The visual representation shows multiple parallel pathways (indicated by the重复的线性层和点积注意力块的堆叠), suggesting the multi-head aspect where each pathway processes the inputs independently and combines the results to form the final output \\(h\\). The \"Concat\" block at the top combines the outputs from all heads before passing the results through another linear layer to produce the final output. This design reflects how transformers efficiently handle dependencies through self-attention mechanisms, which are crucial for tasks involving sequential data like natural language processing.\n",
      "Creating embeddings for all content...\n",
      "Added 59 items to vector store (56 text chunks, 3 image captions)\n",
      "\n",
      "Processing document for text-only RAG...\n",
      "Extracting content from data/attention_is_all_you_need.pdf...\n",
      "Extracted 15 text segments and 3 images\n",
      "Created 56 text chunks\n",
      "Creating embeddings for text-only content...\n",
      "Added 56 text items to text-only vector store\n",
      "\n",
      "\n",
      "=== Evaluating Query 1: What is the BLEU score of the Transformer (base model)? ===\n",
      "\n",
      "Running multi-modal RAG...\n",
      "\n",
      "=== Processing query: What is the BLEU score of the Transformer (base model)? ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rs/vkwl502n4fngfpthv2h_d42r0000gn/T/ipykernel_70964/2117883450.py:75: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  \"similarity\": float(score)  # Convert to float for JSON serialization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5 relevant items (5 text, 0 image captions)\n",
      "\n",
      "Running text-only RAG...\n",
      "\n",
      "=== Processing query: What is the BLEU score of the Transformer (base model)? ===\n",
      "\n",
      "Retrieved 5 relevant items (5 text, 0 image captions)\n",
      "\n",
      "=== OVERALL ANALYSIS ===\n",
      "\n",
      "\n",
      "\n",
      "### **Comprehensive Analysis of Multi-Modal vs. Text-Only RAG**\n",
      "\n",
      "#### **1. Types of Queries Where Multi-Modal RAG Outperforms Text-Only RAG**\n",
      "Multi-modal RAG (text + images) excels in **queries that require contextual understanding of both textual and visual information**. For example:\n",
      "- **Image-based questions**: \"What is the main subject in this image?\" or \"Describe the visual elements in the image.\"\n",
      "- **Cross-modal reasoning**: \"What is the relationship between the text and the image in this document?\" or \"Explain the concept illustrated in this diagram.\"\n",
      "- **Contextual queries**: \"What does the chart in the paper show about the model's performance?\" (where the chart is an image and the text provides labels/annotations).\n",
      "\n",
      "In contrast, **text-only RAG is superior for purely factual, numerical, or abstract queries** (e.g., \"What is the BLEU score of the Transformer (base model)?\"), as seen in the provided example. These queries rely on precise textual data retrieval and do not benefit from image analysis.\n",
      "\n",
      "---\n",
      "\n",
      "#### **2. Specific Advantages of Incorporating Image Information**\n",
      "- **Enhanced contextual understanding**: When queries involve images (e.g., charts, diagrams, or annotated visuals), image captions or visual features can provide critical context that text alone cannot. For example, a query about a \"loss curve in a paper\" would benefit from analyzing the image of the chart.\n",
      "- **Reduced ambiguity**: Visual data can clarify ambiguous textual descriptions. For instance, if a text mentions \"a graph showing accuracy trends,\" the corresponding image could resolve whether the graph is line, bar, or scatter-based.\n",
      "- **Multimodal reasoning**: For tasks requiring cross-modal alignment (e.g., \"What does the image caption say about the model's architecture?\"), combining text and image data enables more accurate answers.\n",
      "\n",
      "**In the given example**, the query about the BLEU score is purely numerical and text-based. Since no image captions were retrieved, the multi-modal approach added no value, and the text-only RAG outperformed it.\n",
      "\n",
      "---\n",
      "\n",
      "#### **3. Disadvantages or Limitations of the Multi-Modal Approach**\n",
      "- **Increased complexity and computational cost**: Processing both text and images requires more resources (e.g., vision models, cross-attention mechanisms), which can slow down inference and increase latency.\n",
      "- **Redundancy in text-only queries**: When queries do not involve images, the multi-modal system may retrieve irrelevant image captions, leading to noise or confusion. In the example, the system retrieved 5 text chunks and 0 image captions, but the absence of images meant the multi-modal approach was redundant.\n",
      "- **Dependency on image quality and relevance**: If the retrieved images are low-resolution, misaligned, or irrelevant, the system may generate incorrect or incomplete answers. For example, a query about \"the methodology in this paper\" would fail if the retrieved image is unrelated to the methodology section.\n",
      "\n",
      "---\n",
      "\n",
      "#### **4. Overall Recommendation: When to Use Each Approach**\n",
      "- **Use Text-Only RAG for**:\n",
      "  - **Factual, numerical, or abstract queries** (e.g., \"What is the BLEU score of the Transformer?\").\n",
      "  - **Scenarios where image data is irrelevant or unavailable**.\n",
      "  - **High-performance requirements** (e.g., low-latency applications, resource-constrained environments).\n",
      "\n",
      "- **Use Multi-Modal RAG for**:\n",
      "  - **Queries involving images, diagrams, or visual data** (e.g., \"What does the chart in the paper show?\").\n",
      "  - **Tasks requiring cross-modal reasoning** (e.g., \"Explain the relationship between the text and the image in this document\").\n",
      "  - **Applications where visual context is critical to answer accuracy** (e.g., medical imaging, product descriptions with images).\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Takeaways**\n",
      "- **Multi-modal RAG is not a universal improvement** over text-only RAG. Its effectiveness depends on the **nature of the query** and the **availability of relevant image data**.\n",
      "- **Text-only RAG remains superior for text-centric tasks**, while multi-modal RAG adds value only when **visual information is necessary or complementary**.\n",
      "- **Design considerations**: For multi-modal systems, ensure that image retrieval and processing are optimized to avoid redundancy and noise, especially in text-only query scenarios.\n"
     ]
    }
   ],
   "source": [
    "# Path to your PDF document\n",
    "pdf_path = \"data/attention_is_all_you_need.pdf\"\n",
    "\n",
    "# Define test queries targeting both text and visual content\n",
    "test_queries = [\n",
    "    \"What is the BLEU score of the Transformer (base model)?\",\n",
    "]\n",
    "\n",
    "# Optional reference answers for evaluation\n",
    "reference_answers = [\n",
    "    \"The Transformer (base model) achieves a BLEU score of 27.3 on the WMT 2014 English-to-German translation task and 38.1 on the WMT 2014 English-to-French translation task.\",\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_multimodal_vs_textonly(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")\n",
    "\n",
    "# Print overall analysis\n",
    "print(\"\\n=== OVERALL ANALYSIS ===\\n\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:0\n",
      "multimodal_response:\n",
      "\n",
      "The BLEU scores for the **Transformer (base model)** are reported in **Table 2** of the retrieved content. Specifically:  \n",
      "- **English-to-German (EN-DE): 27.3 BLEU**  \n",
      "- **English-to-French (EN-FR): 38.1 BLEU**  \n",
      "\n",
      "These scores are achieved at a significantly lower training cost compared to other models listed in the table. The base model's performance is further contextualized in the text, which notes that it surpasses all previously published models and ensembles for both translation tasks, though the **big model** (with higher parameters and training cost) achieves higher BLEU scores (28.4 for EN-DE and 41.8 for EN-FR).  \n",
      "\n",
      "The data is derived from the **newstest2014** benchmark for both tasks.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "for result in evaluation_results[\"results\"]:\n",
    "    print(\"index:\"+str(index))\n",
    "    print(\"multimodal_response:\"+result[\"multimodal_response\"])\n",
    "    print(\"--------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
