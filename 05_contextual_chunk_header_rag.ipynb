{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单 RAG 中的上下文块头 （CCH）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检索增强生成 （RAG） 通过在生成响应之前检索相关的外部知识来提高语言模型的事实准确性。但是，标准分块通常会丢失重要的上下文，从而使检索效率降低。\n",
    "\n",
    "上下文块标头 （CCH） 通过在嵌入之前为每个块预置高级上下文（如文档标题或章节标题）来增强 RAG。这可以提高检索质量并防止脱离上下文的响应。\n",
    "\n",
    "### 步骤：\n",
    "1. 数据接入：加载并预处理文本数据。\n",
    "2. 使用上下文标题进行分块：提取章节标题并将其添加到块的前面。\n",
    "3. 嵌入创建：将上下文增强的块转换为数字表示。\n",
    "4. 语义搜索：根据用户查询检索相关块。\n",
    "5. 响应生成：使用语言模型从检索到的文本中生成响应。\n",
    "6. 评估：使用评分系统评估响应准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import fitz\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.siliconflow.cn/v1/\",\n",
    "    api_key=os.getenv(\"SILLICONFLOW_API_KEY\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 根据上下文头来切分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_header_chunks(chunks, model=\"Qwen/Qwen3-8B\"):\n",
    "    system_prompt = f\"\"\"Generate a concise and informative title for each text of the given text arrays.\n",
    "    response format is json array, each item is a json object with the following fields:\n",
    "    - index: <index of the text in the text array>\n",
    "    - header: <header of the text>\n",
    "    \"\"\"\n",
    "    text_array = \"\"\n",
    "    for chunk in chunks:\n",
    "        text_array += f\"index: {chunk['index']}, text: {chunk['text']}\\n\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Generate header for each text in the following text array: \n",
    "    {text_array} \n",
    "    \"\"\"\n",
    "    print(f\"user_prompt: {user_prompt[:100]} ...\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.1,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_with_headers(text, n, overlap):\n",
    "    \"\"\"\n",
    "    将文本分块，每个块包含n个字符，重叠n-overlap个字符。\n",
    "    \n",
    "    Args:\n",
    "        text: 需要分块的文本\n",
    "        n: 每个块的字符数\n",
    "        overlap: 重叠的字符数\n",
    "    \n",
    "    Returns:\n",
    "        list: 分块后的文本列表\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    step_size = n - overlap\n",
    "    end = len(text)\n",
    "    \n",
    "    index = 0\n",
    "    for i in range(0, end, step_size):\n",
    "        start = i\n",
    "        chunk = text[i:i+step_size]\n",
    "        if(len(chunk.strip()) == 0):\n",
    "            continue\n",
    "        \n",
    "        chunks.append({\n",
    "            \"index\": index,\n",
    "            \"text\": chunk\n",
    "        })\n",
    "        index += 1\n",
    "    if start < end:\n",
    "        chunk = text[start:]\n",
    "        if len(chunk.strip()) != 0:\n",
    "            chunks.append({\n",
    "                \"index\": index,\n",
    "                \"text\": chunk\n",
    "            })\n",
    "    print(f\"chunking length: {len(chunks)}\")\n",
    "    \n",
    "    response = generate_header_chunks(chunks)\n",
    "    \"\"\"\n",
    "    response:\n",
    "    [\n",
    "    {\n",
    "        \"index\": 0,\n",
    "        \"header\": \"Introduction to Artificial Intelligence\"\n",
    "    },\n",
    "    ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    res_json = json.loads(response)\n",
    "    print(f\"generated headers length: {len(res_json)}\")\n",
    "    length = min(len(chunks), len(res_json))\n",
    "    for i in range(length):\n",
    "        results.append({\n",
    "            \"header\": res_json[i][\"header\"],\n",
    "            \"text\": chunks[i][\"text\"]\n",
    "        })\n",
    "  \n",
    "    return results\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunking length: 43\n",
      "user_prompt: Generate header for each text in the following text array: \n",
      "    index: 0, text: Understanding Artifi ...\n",
      "generated headers length: 24\n",
      "Sample Chunk 3\n",
      "Header: Deep Learning and Natural Language Processing: Advanced AI Techniques\n",
      "Text: trained on unlabeled data, where the algorithm must \n",
      "discover patterns and structures in the data without explicit guidance. Common techniques \n",
      "include clustering (grouping similar data points) and dimensionality reduction (reducing the \n",
      "number of variables while preserving important information). \n",
      " \n",
      "Reinforcement Learning \n",
      "Reinforcement learning involves training an agent to make decisions in an environment to \n",
      "maximize a reward. The agent learns through trial and error, receiving feedback in the form of \n",
      "rewards or penalties. This approach is used in game playing, robotics, and resource \n",
      "management. \n",
      "Deep Learning \n",
      "Deep learning is a subfield of machine learning that uses artificial neural networks with multiple \n",
      "layers (deep neural networks) to analyze data. These networks are inspired \n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "chunks = chunk_text_with_headers(extracted_text, 1000, 200)\n",
    "index = 3\n",
    "print(f\"Sample Chunk {index}\")\n",
    "print(\"Header:\", chunks[index][\"header\"])\n",
    "print(\"Text:\", chunks[index][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"BAAI/bge-m3\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text.\n",
    "\n",
    "    Args:\n",
    "    text (str): The input text to be embedded.\n",
    "    model (str): The embedding model to be used. Default is \"BAAI/bge-en-icl\".\n",
    "\n",
    "    Returns:\n",
    "    dict: The response containing the embedding for the input text.\n",
    "    \"\"\"\n",
    "    # Create embeddings using the specified model and input text\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "    # Return the embedding from the response\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:29<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for each chunk\n",
    "embeddings = []  # Initialize an empty list to store embeddings\n",
    "\n",
    "# Iterate through each text chunk with a progress bar\n",
    "for chunk in tqdm(chunks, desc=\"Generating embeddings\"):\n",
    "    # Create an embedding for the chunk's text\n",
    "    text_embedding = create_embeddings(chunk[\"text\"])\n",
    "    # Create an embedding for the chunk's header\n",
    "    header_embedding = create_embeddings(chunk[\"header\"])\n",
    "    # Append the chunk's header, text, and their embeddings to the list\n",
    "    embeddings.append({\"header\": chunk[\"header\"], \"text\": chunk[\"text\"], \"embedding\": text_embedding, \"header_embedding\": header_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): First vector.\n",
    "    vec2 (np.ndarray): Second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: Cosine similarity score.\n",
    "    \"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, chunks, k=5):\n",
    "    \"\"\"\n",
    "    Searches for the most relevant chunks based on a query.\n",
    "\n",
    "    Args:\n",
    "    query (str): User query.\n",
    "    chunks (List[dict]): List of text chunks with embeddings.\n",
    "    k (int): Number of top results.\n",
    "\n",
    "    Returns:\n",
    "    List[dict]: Top-k most relevant chunks.\n",
    "    \"\"\"\n",
    "    # Create an embedding for the query\n",
    "    query_embedding = create_embeddings(query)\n",
    "\n",
    "    similarities = []  # Initialize a list to store similarity scores\n",
    "    \n",
    "    # Iterate through each chunk to calculate similarity scores\n",
    "    for chunk in chunks:\n",
    "        # Compute cosine similarity between query embedding and chunk text embedding\n",
    "        sim_text = cosine_similarity(np.array(query_embedding), np.array(chunk[\"embedding\"]))\n",
    "        # Compute cosine similarity between query embedding and chunk header embedding\n",
    "        sim_header = cosine_similarity(np.array(query_embedding), np.array(chunk[\"header_embedding\"]))\n",
    "        # Calculate the average similarity score\n",
    "        avg_similarity = (sim_text + sim_header) / 2\n",
    "        print(f\"avg_similarity: {avg_similarity}\")\n",
    "        # Append the chunk and its average similarity score to the list\n",
    "        similarities.append((chunk, avg_similarity))\n",
    "\n",
    "    # Sort the chunks based on similarity scores in descending order\n",
    "    # x[1] is the similarity score, x[0] is the chunk\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    # Return the top-k most relevant chunks\n",
    "    return [x[0] for x in similarities[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_similarity: 0.5141255182627062\n",
      "avg_similarity: 0.4861915784763715\n",
      "avg_similarity: 0.48519307808450063\n",
      "avg_similarity: 0.41353554937566883\n",
      "avg_similarity: 0.453214371133864\n",
      "avg_similarity: 0.4681727856968765\n",
      "avg_similarity: 0.4833216155728286\n",
      "avg_similarity: 0.45704598346023007\n",
      "avg_similarity: 0.4707456695938005\n",
      "avg_similarity: 0.5203494308864659\n",
      "avg_similarity: 0.5298433979651435\n",
      "avg_similarity: 0.48241328650322085\n",
      "avg_similarity: 0.487565906902785\n",
      "avg_similarity: 0.47416048692829915\n",
      "avg_similarity: 0.45119198567727997\n",
      "avg_similarity: 0.4673251069267104\n",
      "avg_similarity: 0.47950523840176257\n",
      "avg_similarity: 0.4726999455339618\n",
      "avg_similarity: 0.4347077375422013\n",
      "avg_similarity: 0.4532016003143917\n",
      "avg_similarity: 0.4539905446226156\n",
      "avg_similarity: 0.468996053287773\n",
      "avg_similarity: 0.5001564927782303\n",
      "avg_similarity: 0.4703719502064978\n",
      "Current Query: What is 'Explainable AI' and why is it considered important?\n",
      "Header 1: AI and the Future of Work: Automation, Reskilling, and New Opportunities\n",
      "Content:\n",
      "control, accountability, and the \n",
      "potential for unintended consequences. Establishing clear guidelines and ethical frameworks for \n",
      "AI development and deployment is crucial. \n",
      "Weaponization of AI \n",
      "The potential use of AI in autonomous weapons systems raises significant ethical and security \n",
      "concerns. International discussions and regulations are needed to address the risks associated \n",
      "with AI-powered weapons. \n",
      "Chapter 5: The Future of Artificial Intelligence \n",
      "The future of AI is likely to be characterized by continued advancements and broader adoption \n",
      "across various domains. Key trends and areas of development include: \n",
      "Explainable AI (XAI) \n",
      "Explainable AI (XAI) aims to make AI systems more transparent and understandable. XAI \n",
      "techniques are being developed to provide insights into how AI m\n",
      "\n",
      "Header 2: AI in Business and Industry: Transforming Operations and Services\n",
      "Content:\n",
      "inability \n",
      "Many AI systems, particularly deep learning models, are \"black boxes,\" making it difficult to \n",
      "understand how they arrive at their decisions. Enhancing transparency and explainability is \n",
      "crucial for building trust and accountability. \n",
      " \n",
      " \n",
      "Privacy and Security \n",
      "AI systems often rely on large amounts of data, raising concerns about privacy and data security. \n",
      "Protecting sensitive information and ensuring responsible data handling are essential. \n",
      "Job Displacement \n",
      "The automation capabilities of AI have raised concerns about job displacement, particularly in \n",
      "industries with repetitive or routine tasks. Addressing the potential economic and social impacts \n",
      "of AI-driven automation is a key challenge. \n",
      "Autonomy and Control \n",
      "As AI systems become more autonomous, questions arise about \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load validation data\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "query_index = 0\n",
    "query = data[query_index]['question']\n",
    "\n",
    "# Retrieve the top 2 most relevant text chunks\n",
    "top_chunks = semantic_search(query, embeddings, k=2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Current Query:\", query)\n",
    "for i, chunk in enumerate(top_chunks):  \n",
    "    print(f\"Header {i+1}: {chunk['header']}\")\n",
    "    print(f\"Content:\\n{chunk['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_prompt length:  1838\n",
      "\n",
      "\n",
      "Explainable AI (XAI) refers to techniques designed to make AI systems more transparent and understandable, enabling users to comprehend how AI arrives at its decisions. It is considered important because many AI systems, particularly deep learning models, operate as \"black boxes,\" making their decision-making processes opaque. Enhancing transparency and explainability is crucial for building trust, ensuring accountability, and addressing ethical concerns related to AI's impact on privacy, security, and societal trust.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\"\"\n",
    "\n",
    "def generate_response(system_prompt, user_query, model=\"Qwen/Qwen3-8B\"):\n",
    "    \"\"\"Generate a response to a user query based on the given system prompt and model.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_query},\n",
    "        ],\n",
    "    )\n",
    "    return response\n",
    "\n",
    "user_prompt = \"\\n\".join(f\"header:{chunk['header']}\\n content:{chunk['text']}\\n\" for i, chunk in enumerate(top_chunks))\n",
    "user_prompt = f\"{user_prompt}\\n问题：{query}\"\n",
    "print(\"user_prompt length: \", len(user_prompt))\n",
    "ai_response = generate_response(system_prompt, user_prompt)\n",
    "print(ai_response.choices[0].message.content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  What is 'Explainable AI' and why is it considered important?\n",
      "True Answer:  Explainable AI (XAI) aims to make AI systems more transparent and understandable, providing insights into how they make decisions. It's considered important for building trust, accountability, and ensuring fairness in AI systems.\n",
      "AI Assistant's Response:  \n",
      "\n",
      "Explainable AI (XAI) refers to techniques designed to make AI systems more transparent and understandable, enabling users to comprehend how AI arrives at its decisions. It is considered important because many AI systems, particularly deep learning models, operate as \"black boxes,\" making their decision-making processes opaque. Enhancing transparency and explainability is crucial for building trust, ensuring accountability, and addressing ethical concerns related to AI's impact on privacy, security, and societal trust.\n",
      "Evaluation Response: \n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "evaluate_system_prompt = \"\"\"You are an intelligent evaluation system. \n",
    "Assess the AI assistant's response based on the provided context. \n",
    "- Assign a score of 1 if the response is very close to the true answer. \n",
    "- Assign a score of 0.5 if the response is partially correct. \n",
    "- Assign a score of 0 if the response is incorrect.\n",
    "Return only the score (0, 0.5, or 1).\"\"\"\n",
    "\n",
    "true_answer = data[query_index]['ideal_answer']\n",
    "\n",
    "evaluate_prompt = f\"\"\"\n",
    "User Query: {query}\n",
    "True Answer: {true_answer}\n",
    "AI Assistant's Response: {ai_response}\n",
    "{evaluate_system_prompt}\n",
    "\"\"\"\n",
    "eval_response = generate_response(evaluate_system_prompt, evaluate_prompt)\n",
    "print(\"Query: \", query)\n",
    "print(\"True Answer: \", true_answer)\n",
    "print(\"AI Assistant's Response: \", ai_response.choices[0].message.content)\n",
    "print(\"Evaluation Response:\", eval_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
