{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1595655a",
   "metadata": {},
   "source": [
    "# 增强型 RAG 系统的重新排序\n",
    "\n",
    "此笔记本实施了重新排序技术，以提高 RAG 系统中的检索质量。重新排名是初始检索后的第二个筛选步骤，以确保使用最相关的内容来生成响应。\n",
    "\n",
    "## 重新排名的关键概念\n",
    "\n",
    "1. 初始检索：使用基本相似度搜索的第一步（准确性较低但速度更快）\n",
    "2. 文档评分：评估每个检索到的文档与查询的相关性\n",
    "3. 重新排序：按相关度分数对文档进行排序\n",
    "4. 选择：仅使用最相关的文档来生成响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320ec445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87b77bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Get the page\n",
    "        text = page.get_text(\"text\")  # Extract text from the page\n",
    "        all_text += text  # Append the extracted text to the all_text string\n",
    "\n",
    "    return all_text  # Return the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a353851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "323d4a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://api.siliconflow.cn/v1/\",\n",
    "    api_key=os.getenv(\"SILLICONFLOW_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddf68094",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the vector store.\n",
    "        \"\"\"\n",
    "        self.vectors = []  # List to store embedding vectors\n",
    "        self.texts = []  # List to store original texts\n",
    "        self.metadata = []  # List to store metadata for each text\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "\n",
    "        Args:\n",
    "        text (str): The original text.\n",
    "        embedding (List[float]): The embedding vector.\n",
    "        metadata (dict, optional): Additional metadata.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n",
    "        self.texts.append(text)  # Add the original text to texts list\n",
    "        self.metadata.append(metadata or {})  # Add metadata to metadata list, use empty dict if None\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): Query embedding vector.\n",
    "        k (int): Number of results to return.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: Top k most similar items with their texts and metadata.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return empty list if no vectors are stored\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # Compute cosine similarity between query vector and stored vector\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # Append index and similarity score\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # Add the corresponding text\n",
    "                \"metadata\": self.metadata[idx],  # Add the corresponding metadata\n",
    "                \"similarity\": score  # Add the similarity score\n",
    "            })\n",
    "        \n",
    "        return results  # Return the list of top k similar items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d37bb21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"BAAI/bge-m3\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text using the specified OpenAI model.\n",
    "\n",
    "    Args:\n",
    "    text (str): The input text for which embeddings are to be created.\n",
    "    model (str): The model to be used for creating embeddings.\n",
    "\n",
    "    Returns:\n",
    "    List[float]: The embedding vector.\n",
    "    \"\"\"\n",
    "    # Handle both string and list inputs by converting string input to a list\n",
    "    input_text = text if isinstance(text, list) else [text]\n",
    "    \n",
    "    # Create embeddings for the input text using the specified model\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=input_text\n",
    "    )\n",
    "    \n",
    "    # If input was a string, return just the first embedding\n",
    "    if isinstance(text, str):\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    # Otherwise, return all embeddings as a list of vectors\n",
    "    return [item.embedding for item in response.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a2ee5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for RAG.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "    chunk_size (int): Size of each chunk in characters.\n",
    "    chunk_overlap (int): Overlap between chunks in characters.\n",
    "\n",
    "    Returns:\n",
    "    SimpleVectorStore: A vector store containing document chunks and their embeddings.\n",
    "    \"\"\"\n",
    "    # Extract text from the PDF file\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Chunk the extracted text\n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    # Create embeddings for the text chunks\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # Initialize a simple vector store\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # Add each chunk and its embedding to the vector store\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "901bf446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_with_llm(query, results, top_n=3, model=\"Qwen/Qwen3-8B\"):\n",
    "    \"\"\"\n",
    "    Reranks search results using LLM relevance scoring.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        results (List[Dict]): Initial search results\n",
    "        top_n (int): Number of results to return after reranking\n",
    "        model (str): Model to use for scoring\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Reranked results\n",
    "    \"\"\"\n",
    "    print(f\"Reranking {len(results)} documents... with LLM model {model}\")  # Print the number of documents to be reranked\n",
    "    \n",
    "    scored_results = []  # Initialize an empty list to store scored results\n",
    "    \n",
    "    # Define the system prompt for the LLM\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert at evaluating document relevance for search queries.\n",
    "    Your task is to rate documents on a scale from 0 to 10 based on how well they answer the given query.\n",
    "\n",
    "    Guidelines:\n",
    "    - Score 0-2: Document is completely irrelevant\n",
    "    - Score 3-5: Document has some relevant information but doesn't directly answer the query\n",
    "    - Score 6-8: Document is relevant and partially answers the query\n",
    "    - Score 9-10: Document is highly relevant and directly answers the query\n",
    "\n",
    "    You MUST respond with ONLY a single integer score between 0 and 10. Do not include ANY other text.\n",
    "    \"\"\"\n",
    "    print(\"results[0]: \", results[0])\n",
    "    # Iterate through each result\n",
    "    for i, result in enumerate(results):\n",
    "        # Show progress every 5 documents\n",
    "        if i % 5 == 0:\n",
    "            print(f\"Scoring document {i+1}/{len(results)}...\")\n",
    "        \n",
    "        # Define the user prompt for the LLM\n",
    "        user_prompt = f\"\"\"\n",
    "        Query: {query}\n",
    "\n",
    "        Document:\n",
    "        {result['text']}\n",
    "\n",
    "        Rate this document's relevance to the query on a scale from 0 to 10:\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the LLM response\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Extract the score from the LLM response\n",
    "        score_text = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Use regex to extract the numerical score\n",
    "        score_match = re.search(r'\\b(10|[0-9])\\b', score_text)\n",
    "        if score_match:\n",
    "            score = float(score_match.group(1))\n",
    "        else:\n",
    "            # If score extraction fails, use similarity score as fallback\n",
    "            print(f\"Warning: Could not extract score from response: '{score_text}', using similarity score instead\")\n",
    "            score = result[\"similarity\"] * 10\n",
    "        \n",
    "        # Append the scored result to the list\n",
    "        scored_results.append({\n",
    "            \"text\": result[\"text\"],\n",
    "            \"metadata\": result[\"metadata\"],\n",
    "            \"similarity\": result[\"similarity\"],\n",
    "            \"relevance_score\": score\n",
    "        })\n",
    "    \n",
    "    # Sort results by relevance score in descending order\n",
    "    reranked_results = sorted(scored_results, key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "    \n",
    "    # Return the top_n results\n",
    "    return reranked_results[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "525a8991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_with_keywords(query, results, top_n=3):\n",
    "    \"\"\"\n",
    "    A simple alternative reranking method based on keyword matching and position.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        results (List[Dict]): Initial search results\n",
    "        top_n (int): Number of results to return after reranking\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Reranked results\n",
    "    \"\"\"\n",
    "    print(f\"Reranking {len(results)} documents... with keyword matching\")  # Print the number of documents to be reranked\n",
    "    print(\"results[0]:  \", results[0])\n",
    "    # Extract important keywords from the query\n",
    "    keywords = [word.lower() for word in query.split() if len(word) > 3]\n",
    "    \n",
    "    scored_results = []  # Initialize a list to store scored results\n",
    "    \n",
    "    for result in results:\n",
    "        document_text = result[\"text\"].lower()  # Convert document text to lowercase\n",
    "        \n",
    "        # Base score starts with vector similarity\n",
    "        base_score = result[\"similarity\"] * 0.5\n",
    "        \n",
    "        # Initialize keyword score\n",
    "        keyword_score = 0\n",
    "        for keyword in keywords:\n",
    "            if keyword in document_text:\n",
    "                # Add points for each keyword found\n",
    "                keyword_score += 0.1\n",
    "                \n",
    "                # Add more points if keyword appears near the beginning\n",
    "                first_position = document_text.find(keyword)\n",
    "                if first_position < len(document_text) / 4:  # In the first quarter of the text\n",
    "                    keyword_score += 0.1\n",
    "                \n",
    "                # Add points for keyword frequency\n",
    "                frequency = document_text.count(keyword)\n",
    "                keyword_score += min(0.05 * frequency, 0.2)  # Cap at 0.2\n",
    "        \n",
    "        # Calculate the final score by combining base score and keyword score\n",
    "        final_score = base_score + keyword_score\n",
    "        \n",
    "        # Append the scored result to the list\n",
    "        scored_results.append({\n",
    "            \"text\": result[\"text\"],\n",
    "            \"metadata\": result[\"metadata\"],\n",
    "            \"similarity\": result[\"similarity\"],\n",
    "            \"relevance_score\": final_score\n",
    "        })\n",
    "    \n",
    "    # Sort results by final relevance score in descending order\n",
    "    reranked_results = sorted(scored_results, key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "    \n",
    "    # Return the top_n results\n",
    "    return reranked_results[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f4809e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model=\"Qwen/Qwen3-8B\"):\n",
    "    \"\"\"\n",
    "    Generates a response based on the query and context.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): Retrieved context\n",
    "        model (str): Model to use for response generation\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI's behavior\n",
    "    system_prompt = \"\"\"\n",
    "    You are a helpful AI assistant.\n",
    "    Answer the user's question based only on the provided context. \n",
    "    If you cannot find the answer in the context, state that you don't have enough information.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the user prompt by combining the context and query\n",
    "    user_prompt = f\"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Please provide a comprehensive answer based only on the context above.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the response using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Return the generated response content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94ed0053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_reranking(query, vector_store, reranking_method=\"llm\", top_n=3, model=\"Qwen/Qwen3-8B\"):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline incorporating reranking.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        reranking_method (str): Method for reranking ('llm' or 'keywords')\n",
    "        top_n (int): Number of results to return after reranking\n",
    "        model (str): Model for response generation\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including query, context, and response\n",
    "    \"\"\"\n",
    "    # Create query embedding\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # Initial retrieval (get more than we need for reranking)\n",
    "    initial_results = vector_store.similarity_search(query_embedding, k=10)\n",
    "    \n",
    "    print(f\"Get similar {len(initial_results)} documents from vector store\")\n",
    "    # Apply reranking\n",
    "    if reranking_method == \"llm\":\n",
    "        reranked_results = rerank_with_llm(query, initial_results, top_n=top_n)\n",
    "    elif reranking_method == \"keywords\":\n",
    "        reranked_results = rerank_with_keywords(query, initial_results, top_n=top_n)\n",
    "    else:\n",
    "        # No reranking, just use top results from initial retrieval\n",
    "        reranked_results = initial_results[:top_n]\n",
    "    \n",
    "    # Combine context from reranked results\n",
    "    context = \"\\n\\n===\\n\\n\".join([result[\"text\"] for result in reranked_results])\n",
    "    \n",
    "    # Generate response based on context\n",
    "    response = generate_response(query, context, model)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"reranking_method\": reranking_method,\n",
    "        \"initial_results\": initial_results[:top_n],\n",
    "        \"reranked_results\": reranked_results,\n",
    "        \"context\": context,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2912901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation data from a JSON file\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "query_index = 4\n",
    "# Extract the first query from the validation data\n",
    "query = data[query_index]['question']\n",
    "\n",
    "# Extract the reference answer from the validation data\n",
    "reference_answer = data[query_index]['ideal_answer']\n",
    "\n",
    "# pdf_path\n",
    "pdf_path = \"data/AI_Information.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1909473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Comparing retrieval methods...\n",
      "\n",
      "=== STANDARD RETRIEVAL ===\n",
      "Get similar 10 documents from vector store\n",
      "\n",
      "Query: Does AI have the potential to transform the way we live and work?\n",
      "\n",
      "Response:\n",
      "\n",
      "\n",
      "Yes, AI has significant potential to transform both how we live and work, as outlined in the context. Here’s a comprehensive analysis based on the provided information:\n",
      "\n",
      "### **Transformation in Work**  \n",
      "1. **Automation and Efficiency**:  \n",
      "   - AI automates repetitive tasks in industries like finance (e.g., algorithmic trading) and customer service (e.g., chatbots), increasing efficiency and reducing costs.  \n",
      "   - It optimizes business operations by analyzing data, predicting market trends, and streamlining processes, leading to improved decision-making and productivity.  \n",
      "\n",
      "2. **Job Displacement and New Opportunities**:  \n",
      "   - While AI may displace roles involving routine tasks, it also creates new job opportunities in fields like AI development, data science, AI ethics, and training.  \n",
      "   - Reskilling and upskilling initiatives are critical to help workers adapt to evolving roles and collaborate effectively with AI systems.  \n",
      "\n",
      "3. **Human-AI Collaboration**:  \n",
      "   - AI augments human capabilities by handling mundane tasks, providing insights, and supporting decision-making, enabling humans to focus on creative or strategic work.  \n",
      "   - The future of work is envisioned as a partnership between humans and AI, where the latter enhances productivity without fully replacing human roles.  \n",
      "\n",
      "4. **Ethical Considerations**:  \n",
      "   - Ensuring fairness, transparency, and accountability in AI systems is essential to address concerns about worker rights, privacy, and the ethical implications of automation.  \n",
      "\n",
      "---\n",
      "\n",
      "### **Transformation in Daily Life**  \n",
      "1. **Business and Industry**:  \n",
      "   - AI improves customer relationship management (CRM) through personalized experiences, predictive analytics, and automated interactions.  \n",
      "   - Supply chain management benefits from AI-driven demand forecasting, inventory optimization, and logistics streamlining, enhancing resilience and reducing waste.  \n",
      "\n",
      "2. **Creativity and Innovation**:  \n",
      "   - AI acts as a creative tool, generating art, music, literature, and assisting in design processes. It accelerates scientific discovery and cultural innovation.  \n",
      "   - Examples include AI-generated art and music, which challenge traditional notions of creativity and expand possibilities for artistic expression.  \n",
      "\n",
      "3. **Personal and Everyday Applications**:  \n",
      "   - AI-powered systems, such as those with object recognition and obstacle avoidance, are integrated into technologies like robotics, potentially enhancing convenience and safety in daily life.  \n",
      "\n",
      "---\n",
      "\n",
      "### **Conclusion**  \n",
      "The context clearly indicates that AI has the potential to revolutionize both professional and personal spheres. It drives efficiency, creates new opportunities, and fosters innovation, though its implementation requires careful management of ethical and societal challenges. The transformation is not just about replacing human labor but redefining roles and enhancing capabilities through human-AI collaboration.\n"
     ]
    }
   ],
   "source": [
    "# Process document\n",
    "vector_store = process_document(pdf_path)\n",
    "\n",
    "# Example query\n",
    "query = \"Does AI have the potential to transform the way we live and work?\"\n",
    "\n",
    "# Compare different methods\n",
    "print(\"Comparing retrieval methods...\")\n",
    "\n",
    "# 1. Standard retrieval (no reranking)\n",
    "print(\"\\n=== STANDARD RETRIEVAL ===\")\n",
    "standard_results = rag_with_reranking(query, vector_store, reranking_method=\"none\")\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(f\"\\nResponse:\\n{standard_results['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68298237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LLM-BASED RERANKING ===\n",
      "Get similar 10 documents from vector store\n",
      "Reranking 10 documents... with LLM model Qwen/Qwen3-8B\n",
      "results[0]:  {'text': 'agement, algorithmic trading, and \\ncustomer service. AI-powered systems analyze large datasets to identify patterns, predict market \\nmovements, and automate financial processes. \\nChapter 8: AI and the Future of Work \\nAutomation and Job Displacement \\nThe increasing capabilities of AI raise concerns about job displacement, particularly in industries \\nwith repetitive or routine tasks. While AI may automate some jobs, it also creates new \\nopportunities and transforms existing roles. \\nReskilling and Upskilling \\nAddressing the potential impacts of AI on the workforce requires reskilling and upskilling \\ninitiatives. These programs equip workers with the skills needed to adapt to new roles and \\ncollaborate with AI systems. \\nHuman-AI Collaboration \\nThe future of work is likely to involve increased collaboration between humans and AI systems. AI \\ntools can augment human capabilities, automate mundane tasks, and provide insights that \\nsupport decision-making. \\nNew Job Roles \\nThe development and d', 'metadata': {'index': 18, 'source': 'data/AI_Information.pdf'}, 'similarity': 0.65798610544598}\n",
      "Scoring document 1/10...\n",
      "Scoring document 6/10...\n",
      "\n",
      "Query: Does AI have the potential to transform the way we live and work?\n",
      "\n",
      "Response:\n",
      "\n",
      "\n",
      "Yes, AI has significant potential to transform both how we live and work, as outlined in the context. Here’s a comprehensive analysis based on the provided information:\n",
      "\n",
      "### **Transformation in Work**  \n",
      "1. **Automation and Job Displacement**:  \n",
      "   AI can automate repetitive or routine tasks, raising concerns about job displacement in certain industries. However, it also creates new opportunities by transforming existing roles and generating demand for specialized skills (e.g., AI development, data science, and AI ethics).  \n",
      "\n",
      "2. **Human-AI Collaboration**:  \n",
      "   The future of work is envisioned as a partnership between humans and AI systems. AI tools can augment human capabilities, automate mundane tasks, and provide data-driven insights to support decision-making, enhancing productivity and efficiency.  \n",
      "\n",
      "3. **Reskilling and Upskilling**:  \n",
      "   To adapt to AI-driven changes, workers must acquire new skills through reskilling and upskilling programs. This ensures they can collaborate effectively with AI systems and transition into evolving job roles.  \n",
      "\n",
      "4. **New Job Roles**:  \n",
      "   The development and deployment of AI have created entirely new professions, such as AI training, ethical AI oversight, and creative AI application, requiring specialized expertise.  \n",
      "\n",
      "### **Transformation in Daily Life**  \n",
      "1. **Creativity and Innovation**:  \n",
      "   AI is increasingly used as a creative tool, generating art, music, literature, and aiding in scientific discovery. It also streamlines design processes and accelerates innovation across industries.  \n",
      "\n",
      "2. **Social and Environmental Impact**:  \n",
      "   AI for social good initiatives leverage the technology to address global challenges like climate change, poverty, and healthcare disparities, demonstrating its potential to improve quality of life.  \n",
      "\n",
      "3. **Ethical and Regulatory Considerations**:  \n",
      "   The context emphasizes the need for ethical guidelines, fairness, transparency, and privacy protections to ensure AI is developed and deployed responsibly. Regulation and international collaboration will be critical to managing its societal impact.  \n",
      "\n",
      "### **Conclusion**  \n",
      "AI’s transformative potential is evident in its ability to reshape industries, redefine job roles, enhance creativity, and address societal challenges. However, this transformation requires careful management through ethical frameworks, workforce adaptation, and governance to maximize benefits while mitigating risks. The context underscores that AI is not just a tool for efficiency but a catalyst for reimagining how humans interact with technology and each other.\n"
     ]
    }
   ],
   "source": [
    "# 2. LLM-based reranking\n",
    "print(\"\\n=== LLM-BASED RERANKING ===\")\n",
    "llm_results = rag_with_reranking(query, vector_store, reranking_method=\"llm\")\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(f\"\\nResponse:\\n{llm_results['response']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8536a941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== KEYWORD-BASED RERANKING ===\n",
      "Get similar 10 documents from vector store\n",
      "Reranking 10 documents... with keyword matching\n",
      "results[0]:   {'text': 'agement, algorithmic trading, and \\ncustomer service. AI-powered systems analyze large datasets to identify patterns, predict market \\nmovements, and automate financial processes. \\nChapter 8: AI and the Future of Work \\nAutomation and Job Displacement \\nThe increasing capabilities of AI raise concerns about job displacement, particularly in industries \\nwith repetitive or routine tasks. While AI may automate some jobs, it also creates new \\nopportunities and transforms existing roles. \\nReskilling and Upskilling \\nAddressing the potential impacts of AI on the workforce requires reskilling and upskilling \\ninitiatives. These programs equip workers with the skills needed to adapt to new roles and \\ncollaborate with AI systems. \\nHuman-AI Collaboration \\nThe future of work is likely to involve increased collaboration between humans and AI systems. AI \\ntools can augment human capabilities, automate mundane tasks, and provide insights that \\nsupport decision-making. \\nNew Job Roles \\nThe development and d', 'metadata': {'index': 18, 'source': 'data/AI_Information.pdf'}, 'similarity': 0.65798610544598}\n",
      "\n",
      "Query: Does AI have the potential to transform the way we live and work?\n",
      "\n",
      "Response:\n",
      "\n",
      "\n",
      "Yes, AI has significant potential to transform both how we live and work, as highlighted across multiple domains in the context:  \n",
      "\n",
      "1. **Work Transformation**:  \n",
      "   - **Automation and Efficiency**: AI enhances productivity by automating repetitive tasks (e.g., manufacturing, logistics, customer service) and optimizing processes (e.g., supply chain management, financial analysis). It enables human-AI collaboration, where AI handles mundane tasks while humans focus on strategic decision-making.  \n",
      "   - **Job Displacement and Creation**: While AI may displace roles in routine-based industries, it also creates new opportunities and transforms existing roles, necessitating reskilling and upskilling initiatives.  \n",
      "   - **New Roles**: AI-driven industries are generating demand for roles in AI development, robotics, data analysis, and human-AI system integration.  \n",
      "\n",
      "2. **Daily Life Improvements**:  \n",
      "   - **Healthcare and Services**: AI-powered robots assist in healthcare (e.g., patient care, diagnostics) and service sectors (e.g., cleaning, delivery), improving accessibility and efficiency.  \n",
      "   - **Personalization**: AI enhances customer experiences through personalized recommendations, sentiment analysis, and automated customer service (e.g., chatbots).  \n",
      "   - **Exploration and Safety**: AI enables robots to navigate complex environments, aiding in exploration, disaster response, and hazardous tasks.  \n",
      "\n",
      "3. **Business and Industry Innovation**:  \n",
      "   - **Operational Efficiency**: AI streamlines business operations by analyzing data for insights, reducing costs, and improving decision-making.  \n",
      "   - **Supply Chain Optimization**: Predictive analytics and automation enhance demand forecasting, inventory management, and logistics resilience.  \n",
      "\n",
      "4. **Ethical and Governance Considerations**:  \n",
      "   The context emphasizes the need for responsible development, thoughtful governance, and balancing innovation with risk mitigation to ensure AI’s benefits are maximized while addressing societal challenges.  \n",
      "\n",
      "In summary, AI’s integration into robotics, business, and daily life underscores its transformative potential, reshaping industries, creating new opportunities, and redefining human roles in both work and everyday activities.\n"
     ]
    }
   ],
   "source": [
    "# 3. Keyword-based reranking\n",
    "print(\"\\n=== KEYWORD-BASED RERANKING ===\")\n",
    "keyword_results = rag_with_reranking(query, vector_store, reranking_method=\"keywords\")\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(f\"\\nResponse:\\n{keyword_results['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe282b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reranking(query, standard_results, reranked_results, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Evaluates the quality of reranked results compared to standard results.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        standard_results (Dict): Results from standard retrieval\n",
    "        reranked_results (Dict): Results from reranked retrieval\n",
    "        reference_answer (str, optional): Reference answer for comparison\n",
    "        \n",
    "    Returns:\n",
    "        str: Evaluation output\n",
    "    \"\"\"\n",
    "    # Define the system prompt for the AI evaluator\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems.\n",
    "    Compare the retrieved contexts and responses from two different retrieval methods.\n",
    "    Assess which one provides better context and a more accurate, comprehensive answer.\"\"\"\n",
    "    \n",
    "    # Prepare the comparison text with truncated contexts and responses\n",
    "    comparison_text = f\"\"\"\n",
    "    Query: {query}\n",
    "\n",
    "    Standard Retrieval Context:\n",
    "    {standard_results['context'][:1000]}... [truncated]\n",
    "\n",
    "    Standard Retrieval Answer:\n",
    "    {standard_results['response']}\n",
    "\n",
    "    Reranked Retrieval Context:\n",
    "    {reranked_results['context'][:1000]}... [truncated]\n",
    "\n",
    "    Reranked Retrieval Answer:\n",
    "    {reranked_results['response']}\n",
    "    \"\"\"\n",
    "\n",
    "    # If a reference answer is provided, include it in the comparison text\n",
    "    if reference_answer:\n",
    "        comparison_text += f\"\"\"\n",
    "\n",
    "        Reference Answer:\n",
    "        {reference_answer}\n",
    "        \"\"\"\n",
    "\n",
    "    # Create the user prompt for the AI evaluator\n",
    "    user_prompt = f\"\"\"\n",
    "        {comparison_text}\n",
    "\n",
    "        Please evaluate which retrieval method provided:\n",
    "        1. More relevant context\n",
    "        2. More accurate answer\n",
    "        3. More comprehensive answer\n",
    "        4. Better overall performance\n",
    "\n",
    "        Provide a detailed analysis with specific examples.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the evaluation response using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Qwen/Qwen3-14B\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Return the evaluation output\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da72768b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUATION RESULTS ===\n",
      "\n",
      "\n",
      "### **Evaluation of Retrieval Methods for the Query: \"Does AI have the potential to transform the way we live and work?\"**\n",
      "\n",
      "---\n",
      "\n",
      "#### **1. More Relevant Context**  \n",
      "**Both retrieval methods provided identical contexts**, which are truncated but focus on **AI's role in automation, job displacement, reskilling, human-AI collaboration, and new job roles** (e.g., finance, customer service, CRM, supply chain, and ethical considerations). Since the context is the same for both methods, **relevance is equal**. However, the **reranked retrieval answer** introduces **new points** (e.g., \"social and environmental impact,\" \"AI for social good initiatives\") not present in the original context. This suggests the reranked method may have **inferred or extrapolated** beyond the provided text, which could reduce relevance if the goal is strict adherence to the given context.\n",
      "\n",
      "---\n",
      "\n",
      "#### **2. More Accurate Answer**  \n",
      "**Standard Retrieval Answer** is **more accurate** because it **strictly aligns with the context**. For example:  \n",
      "- It explicitly references **AI in finance (algorithmic trading, CRM)** and **supply chain management** as mentioned in the context.  \n",
      "- It discusses **job displacement** and **reskilling initiatives** without adding unmentioned details like \"AI for social good\" or \"climate change.\"  \n",
      "\n",
      "**Reranked Retrieval Answer** introduces **contextual gaps** (e.g., \"social and environmental impact,\" \"ethical and regulatory considerations\") that are **not supported by the provided context**. While these are valid topics, the context does not mention **AI's role in addressing global challenges** or **specific ethical frameworks**. This makes the reranked answer **less accurate** in terms of fidelity to the source material.\n",
      "\n",
      "---\n",
      "\n",
      "#### **3. More Comprehensive Answer**  \n",
      "The **reranked retrieval answer** is **slightly more comprehensive** in terms of **structure and depth of analysis**, but this is **compromised by the inclusion of unsupported claims**. For example:  \n",
      "- It dedicates a **separate section to \"social and environmental impact\"**, which the standard answer does not.  \n",
      "- It emphasizes **ethical and regulatory considerations** as a distinct point, while the standard answer groups them under the conclusion.  \n",
      "\n",
      "However, the **standard answer** is **comprehensive within the given context**, covering:  \n",
      "- **Automation and efficiency** in finance and customer service.  \n",
      "- **Job displacement and new roles** (e.g., AI development, data science).  \n",
      "- **Human-AI collaboration** and **reskilling initiatives**.  \n",
      "- **Creativity and innovation** (e.g., AI-generated art, scientific discovery).  \n",
      "- **Ethical considerations** (fairness, transparency, privacy).  \n",
      "\n",
      "The **reranked answer** adds **novelty** (e.g., \"AI for social good\") but **lacks the same level of detail** on **personal applications** (e.g., robotics, object recognition) that the standard answer explicitly mentions. Thus, the **standard answer** is **more comprehensive within the provided context**, while the reranked answer sacrifices accuracy for broader coverage.\n",
      "\n",
      "---\n",
      "\n",
      "#### **4. Better Overall Performance**  \n",
      "**Standard Retrieval Method** outperforms the reranked method in **overall performance** due to:  \n",
      "- **Accuracy**: It avoids introducing unsupported claims, ensuring the answer is grounded in the context.  \n",
      "- **Relevance**: It directly addresses the query by focusing on **work transformation** (automation, job roles, reskilling) and **daily life** (creativity, robotics) as outlined in the context.  \n",
      "- **Clarity**: The standard answer organizes information logically, with clear subpoints under each section (e.g., \"Automation and Efficiency\" vs. \"Human-AI Collaboration\").  \n",
      "\n",
      "The **reranked retrieval answer** is **less reliable** because it **speculates beyond the context** (e.g., \"social and environmental impact\") and **reorders information** in a way that may not reflect the original context's emphasis. While its structure is slightly more nuanced, the **inaccuracy** of extrapolated claims undermines its effectiveness.\n",
      "\n",
      "---\n",
      "\n",
      "### **Summary**  \n",
      "| Criteria               | Standard Retrieval Method | Reranked Retrieval Method |  \n",
      "|------------------------|--------------------------|--------------------------|  \n",
      "| **Relevant Context**   | Equal (same context)     | Equal (same context)     |  \n",
      "| **Accuracy**           | **Higher** (sticks to context) | **Lower** (adds unsupported claims) |  \n",
      "| **Comprehensiveness**  | **Higher** (covers all context points in detail) | Moderate (adds new points but lacks depth on some context elements) |  \n",
      "| **Overall Performance** | **Better** (accurate, relevant, and structured) | **Worse** (less accurate despite broader claims) |  \n",
      "\n",
      "**Conclusion**: The **standard retrieval method** provides a **more accurate and comprehensive answer** based on the given context, making it the **superior choice** for this query. The reranked method, while slightly more structured, introduces inaccuracies by extrapolating beyond the provided information.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the quality of reranked results compared to standard results\n",
    "evaluation = evaluate_reranking(\n",
    "    query=query,  # The user query\n",
    "    standard_results=standard_results,  # Results from standard retrieval\n",
    "    reranked_results=llm_results,  # Results from LLM-based reranking\n",
    "    reference_answer=reference_answer  # Reference answer for comparison\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"\\n=== EVALUATION RESULTS ===\")\n",
    "print(evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
