{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估块大小\n",
    "在RAG中，选择正确的块大小是提升索引准确度的关键。目标是平衡检索性能与响应质量。\n",
    "本篇文章分成\n",
    "1. 从PDF中提取文本\n",
    "2. 将文本切成大小不同的块\n",
    "3. 为每个块创建嵌入\n",
    "4. 根据查询信息，获取相关的块，\n",
    "5. 用相关的索引块，生成回答\n",
    "6. 衡量置信度和相关度\n",
    "7. 对不同块比较不同结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://api.siliconflow.cn/v1/\",\n",
    "    api_key=os.getenv(\"SILLICONFLOW_API_KEY\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past f\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"\n",
    "    for page in mypdf:\n",
    "        all_text += page.get_text(\"text\") + \" \"\n",
    "    \n",
    "    return all_text.strip()\n",
    "\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对提取的文件进行分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一般使用块大小的1/5作为重叠, 但是这个值需要根据实际情况调整\n",
      "分块中... 块大小:  128  重叠:  25\n",
      "一般使用块大小的1/5作为重叠, 但是这个值需要根据实际情况调整\n",
      "分块中... 块大小:  256  重叠:  51\n",
      "一般使用块大小的1/5作为重叠, 但是这个值需要根据实际情况调整\n",
      "分块中... 块大小:  512  重叠:  102\n",
      "分割线-----------------------------------------------------------------------------------------------------\n",
      "块大小: 128 的切分, 块数: 326\n",
      "第一块:  Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers t\n",
      "分割线-----------------------------------------------------------------------------------------------------\n",
      "分割线-----------------------------------------------------------------------------------------------------\n",
      "块大小: 256 的切分, 块数: 164\n",
      "第一块:  Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. \n",
      "分割线-----------------------------------------------------------------------------------------------------\n",
      "分割线-----------------------------------------------------------------------------------------------------\n",
      "块大小: 512 的切分, 块数: 82\n",
      "第一块:  Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past few decades, \n",
      "分割线-----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size, overlap):\n",
    "    chunks = []\n",
    "    # 使用滑动窗口来分块\n",
    "    print(\"一般使用块大小的1/5作为重叠, 但是这个值需要根据实际情况调整\")\n",
    "    print(\"分块中... 块大小: \", chunk_size, \" 重叠: \", overlap) \n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunk = text[i:i+chunk_size]\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "# 定义块大小128, 256, 512\n",
    "chunk_sizes = [128, 256, 512]\n",
    "\n",
    "text_chunks_dict = {size: chunk_text(extracted_text, size, size//5) \n",
    "                    for size in chunk_sizes}\n",
    "for size, chunks in text_chunks_dict.items():\n",
    "    print(\"分割线-\"+\"-\"*100)\n",
    "    print(f\"块大小: {size} 的切分, 块数: {len(chunks)}\")\n",
    "    print(\"第一块: \", chunks[0])\n",
    "    print(\"分割线-\"+\"-\"*100)\n",
    "# 为每个块创建嵌入\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 为文本块创建Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings:   0%|                                                                                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在为文本块创建Embedding..., 文本块数:  326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings:  33%|████████████████████████████████████▎                                                                        | 1/3 [00:12<00:24, 12.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在为文本块创建Embedding..., 文本块数:  164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings:  67%|████████████████████████████████████████████████████████████████████████▋                                    | 2/3 [00:16<00:07,  7.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在为文本块创建Embedding..., 文本块数:  82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:18<00:00,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of chunk embeddings for chunk-size 128 is 326\n",
      "size of chunk embeddings for chunk-size 256 is 164\n",
      "size of chunk embeddings for chunk-size 512 is 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\"\"\"\n",
    "    model: BAAI/bge-m3   context_length: 8k    \n",
    "    model: BAAI/bge-large-en-v1.5    context_length: 0.5k  token: 512\n",
    "\"\"\"\n",
    "def create_embeddings(chunks:List[str], model= \"BAAI/bge-large-en-v1.5\"):\n",
    "    \"\"\"创建文本块的Embedding\n",
    "\n",
    "    Args:\n",
    "        texts (List[str]): 文本块列表\n",
    "\n",
    "    Returns:\n",
    "        embeddings (List[List[float]]): 文本块的Embedding列表\n",
    "    \"\"\"\n",
    "    print(\"正在为文本块创建Embedding..., 文本块数: \", len(chunks))\n",
    "    \n",
    "    # chunks = chunks[:2]\n",
    "    # print(chunks)\n",
    "    Max_chunk_size = 32\n",
    "    batch_chunks = []\n",
    "    batch_size = len(chunks) // Max_chunk_size\n",
    "    if len(chunks) % Max_chunk_size != 0:\n",
    "        batch_size += 1\n",
    "    for i in range(batch_size):\n",
    "        batch_chunks.append(chunks[i*Max_chunk_size:(i+1)*Max_chunk_size])\n",
    "    all_embeddings = []\n",
    "    for batch_chunk in batch_chunks:\n",
    "        response = client.embeddings.create(model=model, input=batch_chunk)\n",
    "        embeddings = [np.array(embedding.embedding) for embedding in response.data]\n",
    "        all_embeddings.extend(embeddings)\n",
    "    return all_embeddings\n",
    "    \n",
    "\n",
    "chunk_embeddings_dict = {size: create_embeddings(chunks) for size, chunks in tqdm(text_chunks_dict.items(), desc=\"Generating Embeddings\")}   \n",
    "for size, embeddings in chunk_embeddings_dict.items():\n",
    "    print(f\"size of chunk embeddings for chunk-size {size} is {len(embeddings)}\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relative_chunks(query, text_chunks, chuck_embeddings, top_k=5):\n",
    "    query_embedding = create_embeddings([query])[0]\n",
    "    similarities = [cosine_similarity(query_embedding, chunk_embedding) for chunk_embedding in chuck_embeddings]\n",
    "    # 获取相似度最高的top_k个块\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    return [text_chunks[i] for i in top_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在为文本块创建Embedding..., 文本块数:  1\n",
      "正在为文本块创建Embedding..., 文本块数:  1\n",
      "正在为文本块创建Embedding..., 文本块数:  1\n",
      "['AI enables personalized medicine by analyzing individual patient data, predicting treatment \\nresponses, and tailoring interventions. Personalized medicine enhances treatment effectiveness \\nand reduces adverse effects. \\nRobotic Surgery \\nAI-powered robotic s', ' analyzing biological data, predicting drug \\nefficacy, and identifying potential drug candidates. AI-powered systems reduce the time and cost \\nof bringing new treatments to market. \\nPersonalized Medicine \\nAI enables personalized medicine by analyzing indiv', 'g \\npatient outcomes, and assisting in treatment planning. AI-powered tools enhance accuracy, \\nefficiency, and patient care. \\nDrug Discovery and Development \\nAI accelerates drug discovery and development by analyzing biological data, predicting drug \\neffica', 'mains. \\nThese applications include: \\nHealthcare \\nAI is transforming healthcare through applications such as medical diagnosis, drug discovery, \\npersonalized medicine, and robotic surgery. AI-powered tools can analyze medical images, \\npredict patient outcom', 't of self-driving cars, advanced medical \\ndiagnostics, and sophisticated financial modeling tools demonstrates the broad and growing \\napplications of AI. Concerns about ethical implications, bias, and job displacement are also \\nincreasingly prominent. \\nCha']\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/val.json\", \"r\") as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "query = val_data[3][\"question\"]\n",
    "retrieved_chunks_dict = {size: retrieve_relative_chunks(query, text_chunks_dict[size], chunk_embeddings_dict[size], top_k=5) for size in chunk_sizes}\n",
    "\n",
    "print(retrieved_chunks_dict[256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "AI contributes to personalized medicine by analyzing individual patient data to predict treatment responses and tailor interventions, thereby enhancing treatment effectiveness and reducing adverse effects. It also aids in drug discovery by predicting drug efficacy, identifying potential candidates, and accelerating development processes. Additionally, AI-powered tools assist in treatment planning, improve accuracy and efficiency, and support better patient outcomes through data-driven decision-making.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that strictly answer the question based on the given context. If the answer cannot be derived from the context, you should answer \"I don't have enough information to answer that.\". \n",
    "\"\"\"\n",
    "def generate_answer(query, system_prompt, retrieved_chunk, model=\"Qwen/Qwen3-8B\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {query}\\nContext: {retrieved_chunk}\"}\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "ai_answer_dict = {size: generate_answer(query, system_prompt, retrieved_chunks_dict[size]) for size in chunk_sizes}\n",
    "\n",
    "print(ai_answer_dict[256])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the AI Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_FULL = 1\n",
    "SCORE_PARTIAL = 0.5\n",
    "SCORE_WRONG = 0\n",
    "\n",
    "FAITHFULNESS_PROMPT_TEMPLATE = \"\"\"\n",
    "Evaluate the faithfulness of the AI response compared to the correct answer.\n",
    "\n",
    "Question: {question}\n",
    "AI Response: {ai_response}\n",
    "Correct Answer: {correct_answer}\n",
    "\n",
    "Faithfulness measures how well the AI response aligns with the correct answer, without hallucinations.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Score STRICTLY using only these values:\n",
    "    - {SCORE_FULL} Completely faithful, no contradictions\n",
    "    - {SCORE_PARTIAL} Partially faithful, minor contradictions\n",
    "    - {SCORE_WRONG} No faithfulness, major contradictions or hallucinations\n",
    "- Return ONLY the score, nothing else.\n",
    "\"\"\"\n",
    "RELEVANCY_PROMPT_TEMPLATE = \"\"\"\n",
    "Evaluate the relevancy of the AI response to the user query.\n",
    "User Query: {question}\n",
    "AI Response: {ai_response}\n",
    "\n",
    "Relevancy measures how well the response addresses the user's question.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Score STRICTLY using only these values:\n",
    "    * {SCORE_FULL} = Completely relevant, directly addresses the query\n",
    "    * {SCORE_PARTIAL} = Partially relevant, addresses some aspects\n",
    "    * {SCORE_WRONG} = Not relevant, fails to address the query\n",
    "- Return ONLY the numerical score ({SCORE_FULL}, {SCORE_PARTIAL}, or {SCORE_WRONG}) with no explanation or additional text.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Score (Chunk Size 256): \n",
      "\n",
      "1\n",
      "Relevancy Score (Chunk Size 256): \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "Faithfulness Score (Chunk Size 128): \n",
      "\n",
      "1\n",
      "Relevancy Score (Chunk Size 128): \n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def evaluate_answer(query, ai_response, correct_answer, model=\"Qwen/Qwen3-8B\"):\n",
    "    faithfulness_prompt = FAITHFULNESS_PROMPT_TEMPLATE.format(\n",
    "        question=query,\n",
    "        ai_response=ai_response,\n",
    "        correct_answer=correct_answer,\n",
    "        SCORE_FULL=SCORE_FULL,\n",
    "        SCORE_PARTIAL=SCORE_PARTIAL,\n",
    "        SCORE_WRONG=SCORE_WRONG\n",
    "    )\n",
    "    relevance_prompt = RELEVANCY_PROMPT_TEMPLATE.format(\n",
    "        question=query,\n",
    "        ai_response=ai_response,\n",
    "        SCORE_FULL=SCORE_FULL,\n",
    "        SCORE_PARTIAL=SCORE_PARTIAL,\n",
    "        SCORE_WRONG=SCORE_WRONG\n",
    "    )\n",
    "    faithfulness_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": faithfulness_prompt}],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    relevance_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": relevance_prompt}],\n",
    "        temperature=0.0\n",
    "    )   \n",
    "    return faithfulness_response.choices[0].message.content, relevance_response.choices[0].message.content\n",
    "\n",
    "true_answer = val_data[3]['ideal_answer']\n",
    "\n",
    "# Evaluate response for chunk size 256 and 128\n",
    "faithfulness, relevancy = evaluate_answer(query, ai_answer_dict[256], true_answer)\n",
    "faithfulness2, relevancy2 = evaluate_answer(query, ai_answer_dict[128], true_answer)\n",
    "\n",
    "# print the evaluation scores\n",
    "print(f\"Faithfulness Score (Chunk Size 256): {faithfulness}\")\n",
    "print(f\"Relevancy Score (Chunk Size 256): {relevancy}\")\n",
    "\n",
    "print(f\"\\n\")\n",
    "\n",
    "print(f\"Faithfulness Score (Chunk Size 128): {faithfulness2}\")\n",
    "print(f\"Relevancy Score (Chunk Size 128): {relevancy2}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
