{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0c50db6",
   "metadata": {},
   "source": [
    "# 通过问题产生的文档增强RAG\n",
    "\n",
    "本笔记本通过通过问题生成使用文档扩展来实现增强的RAG方法。通过为每个文本块产生相关问题，我们改进了检索过程，从而从语言模型中提出了更好的回答。\n",
    "\n",
    "在本实施中，我们遵循以下步骤：\n",
    "\n",
    "1.数据摄入：从PDF文件中提取文本。\n",
    "2.块：将文本分成可管理的块。\n",
    "3.问题生成：为每个块生成相关问题。\n",
    "4.嵌入创建：为块和问题创建嵌入。\n",
    "5.矢量存储创建：使用numpy构建一个简单的矢量存储。\n",
    "6.语义搜索：检索用户查询的相关块和问题。\n",
    "7.响应生成：基于检索的内容生成答案。\n",
    "8.评估：评估生成的响应的质量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1aa402d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d16c4fa",
   "metadata": {},
   "source": [
    "## 提取PDF文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4feed38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Get the page\n",
    "        text = page.get_text(\"text\")  # Extract text from the page\n",
    "        all_text += text  # Append the extracted text to the all_text string\n",
    "\n",
    "    return all_text  # Return the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d253f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f38ec103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.siliconflow.cn/v1/\",\n",
    "    api_key=os.getenv(\"SILLICONFLOW_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9618c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate questions for each chunk\n",
    "def generate_questions(text_chunk, num_questions=5, model=\"Qwen/Qwen3-8B\"):\n",
    "    \"\"\"\n",
    "    Generates relevant questions that can be answered from the given text chunk.\n",
    "\n",
    "    Args:\n",
    "    text_chunk (str): The text chunk to generate questions from.\n",
    "    num_questions (int): Number of questions to generate.\n",
    "    model (str): The model to use for question generation.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: List of generated questions.\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI's behavior\n",
    "    system_prompt = \"You are an expert at generating relevant questions from text. Create concise questions that can be answered using only the provided text. Focus on key information and concepts.\"\n",
    "    \n",
    "    # Define the user prompt with the text chunk and the number of questions to generate\n",
    "    user_prompt = f\"\"\"\n",
    "    Based on the following text, generate {num_questions} different questions that can be answered using only this text:\n",
    "\n",
    "    {text_chunk}\n",
    "    \n",
    "    Format your response as a numbered list of questions only, with no additional text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate questions using the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.7,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Extract and clean questions from the response\n",
    "    questions_text = response.choices[0].message.content.strip()\n",
    "    questions = []\n",
    "    \n",
    "    # Extract questions using regex pattern matching\n",
    "    for line in questions_text.split('\\n'):\n",
    "        # Remove numbering and clean up whitespace\n",
    "        cleaned_line = re.sub(r'^\\d+\\.\\s*', '', line.strip())\n",
    "        if cleaned_line and cleaned_line.endswith('?'):\n",
    "            questions.append(cleaned_line)\n",
    "    \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87018bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Embeddings for chunks and questions\n",
    "def create_embeddings(text, model=\"BAAI/bge-m3\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text.\n",
    "\n",
    "    Args:\n",
    "    text (str): The input text to be embedded.\n",
    "    model (str): The embedding model to be used. Default is \"BAAI/bge-m3\".\n",
    "\n",
    "    Returns:\n",
    "    dict: The response containing the embedding for the input text.\n",
    "    \"\"\"\n",
    "    # Create embeddings using the specified model and input text\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "    # Return the embedding from the response\n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "312799f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vectors Store\n",
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the vector store.\n",
    "        \"\"\"\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "\n",
    "        Args:\n",
    "        text (str): The original text.\n",
    "        embedding (List[float]): The embedding vector.\n",
    "        metadata (dict, optional): Additional metadata.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): Query embedding vector.\n",
    "        k (int): Number of results to return.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: Top k most similar items with their texts and metadata.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54bb8d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Document\n",
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200, questions_per_chunk=5):\n",
    "    \"\"\"\n",
    "    Process a document with question augmentation.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "    chunk_size (int): Size of each text chunk in characters.\n",
    "    chunk_overlap (int): Overlap between chunks in characters.\n",
    "    questions_per_chunk (int): Number of questions to generate per chunk.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[str], SimpleVectorStore]: Text chunks and vector store.\n",
    "    \"\"\"\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"Chunking text...\")\n",
    "    text_chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(text_chunks)} text chunks\")\n",
    "    \n",
    "    vector_store = SimpleVectorStore()\n",
    "    \n",
    "    print(\"Processing chunks and generating questions...\")\n",
    "    for i, chunk in enumerate(tqdm(text_chunks, desc=\"Processing Chunks\")):\n",
    "        # Create embedding for the chunk itself\n",
    "        chunk_embedding_response = create_embeddings(chunk)\n",
    "        chunk_embedding = chunk_embedding_response.data[0].embedding\n",
    "        \n",
    "        # Add the chunk to the vector store\n",
    "        vector_store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=chunk_embedding,\n",
    "            metadata={\"type\": \"chunk\", \"index\": i}\n",
    "        )\n",
    "        \n",
    "        # Generate questions for this chunk\n",
    "        questions = generate_questions(chunk, num_questions=questions_per_chunk)\n",
    "        \n",
    "        # Create embeddings for each question and add to vector store\n",
    "        for j, question in enumerate(questions):\n",
    "            question_embedding_response = create_embeddings(question)\n",
    "            question_embedding = question_embedding_response.data[0].embedding\n",
    "            \n",
    "            # Add the question to the vector store\n",
    "            vector_store.add_item(\n",
    "                text=question,\n",
    "                embedding=question_embedding,\n",
    "                metadata={\"type\": \"question\", \"chunk_index\": i, \"original_chunk\": chunk}\n",
    "            )\n",
    "    \n",
    "    return text_chunks, vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830450b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Processing chunks and generating questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 42/42 [14:49<00:00, 21.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store contains 168 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the PDF file\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# Process the document (extract text, create chunks, generate questions, build vector store)\n",
    "text_chunks, vector_store = process_document(\n",
    "    pdf_path, \n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200, \n",
    "    questions_per_chunk=3\n",
    ")\n",
    "\n",
    "print(f\"Vector store contains {len(vector_store.texts)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9cf875f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store element -1: What qualities are necessary to harness AI's transformative potential according to the text?\n",
      "Vector store element -1 embedding: [-0.02662256  0.01133184 -0.010896   ... -0.00609268 -0.02836592\n",
      "  0.02566008]\n",
      "Vector store element -1 metadata: {'type': 'question', 'chunk_index': 41, 'original_chunk': 'promoting STEM education, providing reskilling and upskilling opportunities, and \\nfostering lifelong learning. \\nA Human-Centered Approach \\nA human-centered approach to AI focuses on developing AI systems that enhance human \\ncapabilities, promote well-being, and align with human values. This involves considering the \\nethical, social, and psychological impacts of AI and prioritizing human needs and interests. \\nBy embracing these principles and working together, we can harness the transformative potential \\nof AI to create a more innovative, equitable, and sustainable future. The path forward requires \\ndedication, collaboration, and a commitment to responsible AI development and deployment. \\n \\n'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Vector store element -1:\", vector_store.texts[-1])\n",
    "print(\"Vector store element -1 embedding:\", vector_store.vectors[-1])\n",
    "print(\"Vector store element -1 metadata:\", vector_store.metadata[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45dac3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Search\n",
    "def semantic_search(query, vector_store, k=5):\n",
    "    \"\"\"\n",
    "    Performs semantic search using the query and vector store.\n",
    "\n",
    "    Args:\n",
    "    query (str): The search query.\n",
    "    vector_store (SimpleVectorStore): The vector store to search in.\n",
    "    k (int): Number of results to return.\n",
    "\n",
    "    Returns:\n",
    "    List[Dict]: Top k most relevant items.\n",
    "    \"\"\"\n",
    "    # Create embedding for the query\n",
    "    query_embedding_response = create_embeddings(query)\n",
    "    query_embedding = query_embedding_response.data[0].embedding\n",
    "    \n",
    "    # Search the vector store\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a70e9a8",
   "metadata": {},
   "source": [
    "# Run An Augmented RAG Query in Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0e471e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How does AI contribute to personalized medicine?\n",
      "\n",
      "Search Results:\n",
      "\n",
      "Relevant Document Chunks:\n",
      "\n",
      "Matched Questions:\n",
      "Question 1 (similarity: 0.8071):\n",
      "What are the benefits of AI in healthcare?\n",
      "From chunk 24\n",
      "=====================================\n",
      "Question 2 (similarity: 0.8066):\n",
      "How does AI improve healthcare administration?\n",
      "From chunk 24\n",
      "=====================================\n",
      "Question 3 (similarity: 0.7760):\n",
      "How does AI contribute to accelerating drug discovery and development?\n",
      "From chunk 23\n",
      "=====================================\n",
      "Question 4 (similarity: 0.7685):\n",
      "In what ways does AI enhance personalized learning in education?\n",
      "From chunk 7\n",
      "=====================================\n",
      "Question 5 (similarity: 0.7639):\n",
      "What are the key applications of AI in healthcare as described in the text?\n",
      "From chunk 23\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "# Load the validation data from a JSON file\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "query_index = 3\n",
    "# Extract the first query from the validation data\n",
    "query = data[query_index]['question']\n",
    "\n",
    "# Perform semantic search to find relevant content\n",
    "search_results = semantic_search(query, vector_store, k=5)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"\\nSearch Results:\")\n",
    "\n",
    "# Organize results by type\n",
    "chunk_results = []\n",
    "question_results = []\n",
    "\n",
    "for result in search_results:\n",
    "    if result[\"metadata\"][\"type\"] == \"chunk\":\n",
    "        chunk_results.append(result)\n",
    "    else:\n",
    "        question_results.append(result)\n",
    "\n",
    "# Print chunk results first\n",
    "print(\"\\nRelevant Document Chunks:\")\n",
    "for i, result in enumerate(chunk_results):\n",
    "    print(f\"Context {i + 1} (similarity: {result['similarity']:.4f}):\")\n",
    "    print(result[\"text\"][:300] + \"...\")\n",
    "    print(\"=====================================\")\n",
    "\n",
    "# Then print question matches\n",
    "print(\"\\nMatched Questions:\")\n",
    "for i, result in enumerate(question_results):\n",
    "    print(f\"Question {i + 1} (similarity: {result['similarity']:.4f}):\")\n",
    "    print(result[\"text\"])\n",
    "    chunk_idx = result[\"metadata\"][\"chunk_index\"]\n",
    "    print(f\"From chunk {chunk_idx}\")\n",
    "    print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2fbf0561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate context using Augmented RAG, MIND to delete the duplicate chunks\n",
    "def prepare_context(search_results):\n",
    "    \"\"\"\n",
    "    Prepares a unified context from search results for response generation.\n",
    "\n",
    "    Args:\n",
    "    search_results (List[Dict]): Results from semantic search.\n",
    "\n",
    "    Returns:\n",
    "    str: Combined context string.\n",
    "    \"\"\"\n",
    "    # Extract unique chunks referenced in the results\n",
    "    chunk_indices = set()\n",
    "    context_chunks = []\n",
    "    \n",
    "    # First add direct chunk matches\n",
    "    for result in search_results:\n",
    "        if result[\"metadata\"][\"type\"] == \"chunk\":\n",
    "            chunk_indices.add(result[\"metadata\"][\"index\"])\n",
    "            context_chunks.append(f\"Chunk {result['metadata']['index']}:\\n{result['text']}\")\n",
    "    \n",
    "    # Then add chunks referenced by questions\n",
    "    for result in search_results:\n",
    "        if result[\"metadata\"][\"type\"] == \"question\":\n",
    "            chunk_idx = result[\"metadata\"][\"chunk_index\"]\n",
    "            if chunk_idx not in chunk_indices:\n",
    "                chunk_indices.add(chunk_idx)\n",
    "                context_chunks.append(f\"Chunk {chunk_idx} (referenced by question '{result['text']}'):\\n{result['metadata']['original_chunk']}\")\n",
    "    \n",
    "    # Combine all context chunks\n",
    "    full_context = \"\\n\\n\".join(context_chunks)\n",
    "    return full_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "00b6b58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model_name=\"Qwen/Qwen3-8B\"):\n",
    "    \"\"\"\n",
    "    Generate a response to a query using a given context and model.\n",
    "\n",
    "    Args:\n",
    "    query (str): The user's query.\n",
    "    context (str): The context to be used for response generation.\n",
    "    model_name (str): The name of the model to be used for response generation.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated response.\n",
    "    \"\"\"\n",
    "    sys_prompt = \"\"\"You are a helpful assistant. You are given a query and a context. \n",
    "    Generate a response to the query derived from the given context directly or based on the context. \n",
    "    Be concise and accurate.\n",
    "    If the query is not related to the context, answer with 'I don't have enough information to answer that.'\"\"\"\n",
    "    user_prompt = \"\"\"Query: {query}\n",
    "    Context: {context}\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt.format(query=query, context=context)},\n",
    "        ],\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "623f9314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How does AI contribute to personalized medicine?\n",
      "Response: \n",
      "\n",
      "AI contributes to personalized medicine by analyzing individual patient data to predict treatment responses and tailor interventions. This approach enhances treatment effectiveness and reduces adverse effects, as highlighted in the context.\n",
      "Context: Chunk 24 (referenced by question 'What are the benefits of AI in healthcare?'):\n",
      "control. These systems enhance dexterity, reduce invasiveness, and \n",
      "improve patient outcomes. \n",
      "Healthcare Administration \n",
      "AI streamlines healthcare administration by automating tasks, managing patient records, and \n",
      "optimizing workflows. AI-powered systems improve efficiency, reduce costs, and enhance \n",
      "patient experience. \n",
      "Chapter 12: AI and Cybersecurity \n",
      "Threat Detection and Prevention \n",
      "AI enhances cybersecurity by detecting and preventing threats, analyzing network traffic, and \n",
      "identifying vulnerabilities. AI-powered systems automate security tasks, improve threat \n",
      "detection accuracy, and enhance overall cybersecurity posture. \n",
      "Anomaly Detection \n",
      "AI-powered anomaly detection systems identify unusual patterns and behaviors that may \n",
      "indicate a security threat. These systems provide real-time alerts and support rapid response to \n",
      "security incidents. \n",
      "Fraud Detection \n",
      "AI is used in fraud detection to analyze transactions, identify suspicious activities, and prevent \n",
      "fraudulent actions.\n",
      "\n",
      "Chunk 23 (referenced by question 'How does AI contribute to accelerating drug discovery and development?'):\n",
      "ent by analyzing medical images, predicting \n",
      "patient outcomes, and assisting in treatment planning. AI-powered tools enhance accuracy, \n",
      "efficiency, and patient care. \n",
      "Drug Discovery and Development \n",
      "AI accelerates drug discovery and development by analyzing biological data, predicting drug \n",
      "efficacy, and identifying potential drug candidates. AI-powered systems reduce the time and cost \n",
      "of bringing new treatments to market. \n",
      "Personalized Medicine \n",
      "AI enables personalized medicine by analyzing individual patient data, predicting treatment \n",
      "responses, and tailoring interventions. Personalized medicine enhances treatment effectiveness \n",
      "and reduces adverse effects. \n",
      "Robotic Surgery \n",
      "AI-powered robotic surgery systems assist surgeons in performing complex procedures with \n",
      "greater precision and control. These systems enhance dexterity, reduce invasiveness, and \n",
      "improve patient outcomes. \n",
      "Healthcare Administration \n",
      "AI streamlines healthcare administration by automating tasks, managing patient\n",
      "\n",
      "Chunk 7 (referenced by question 'In what ways does AI enhance personalized learning in education?'):\n",
      "stomer \n",
      "service chatbots, and supply chain optimization. AI-powered systems can analyze customer data \n",
      "to predict demand, personalize offers, and improve the shopping experience. \n",
      "Manufacturing \n",
      "AI is used in manufacturing for predictive maintenance, quality control, process optimization, \n",
      "and robotics. AI-powered systems can monitor equipment, detect anomalies, and automate \n",
      "tasks, leading to increased efficiency and reduced costs. \n",
      "Education \n",
      "AI is enhancing education through personalized learning platforms, automated grading systems, \n",
      "and virtual tutors. AI-powered tools can adapt to individual student needs, provide feedback, and \n",
      "create customized learning experiences. \n",
      "Entertainment \n",
      "The entertainment industry uses AI for content recommendation, game development, and virtual \n",
      "reality experiences. AI algorithms analyze user preferences to suggest movies, music, and \n",
      "games, enhancing user engagement. \n",
      "Cybersecurity \n",
      "AI is used in cybersecurity to detect and respond to threats, anal\n"
     ]
    }
   ],
   "source": [
    "context = prepare_context(search_results)\n",
    "response = generate_response(query, context)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"Response:\", response.choices[0].message.content)\n",
    "print(\"Context:\", context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b60565d",
   "metadata": {},
   "source": [
    "# Evaluation the answer to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "809c5cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:  How does AI contribute to personalized medicine?\n",
      "ideal_response:  AI enables personalized medicine by analyzing individual patient data, predicting treatment responses, and tailoring interventions to specific needs. This enhances treatment effectiveness and reduces adverse effects.\n",
      "generated_response:  \n",
      "\n",
      "AI contributes to personalized medicine by analyzing individual patient data to predict treatment responses and tailor interventions. This approach enhances treatment effectiveness and reduces adverse effects, as highlighted in the context.\n",
      "evaluation:  \n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "sys_prompt = \"\"\"\n",
    "You are an intelligent evaluation system tasked with assessing the AI assistant's responses.\n",
    "If the AI assistant's response is very close to the true response, assign a score of 1.\n",
    "If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. \n",
    "If the response is partially aligned with the true response, assign a score of 0.5.\n",
    "\"\"\"\n",
    "ideal_response = data[query_index]['ideal_answer']\n",
    "generated_response = response.choices[0].message.content\n",
    "\n",
    "user_prompt = f\"\"\"Query: {query}\n",
    "Ideal Response: {ideal_response}\n",
    "AI Assistant's Response: {generated_response}\"\"\"\n",
    "\n",
    "evaluation_response = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen3-8B\",\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    ")\n",
    "print(\"query: \", query)\n",
    "print(\"ideal_response: \", ideal_response)\n",
    "print(\"generated_response: \", generated_response)\n",
    "print(\"evaluation: \", evaluation_response.choices[0].message.content)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
